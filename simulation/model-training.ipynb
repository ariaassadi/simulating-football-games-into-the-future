{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import load_model as keras_load_model\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from datetime import date\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from visualize_game import visualize_training_results\n",
    "from utils import load_processed_frames, split_match_ids, get_next_model_filename, euclidean_distance_loss, adjust_for_embeddings\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf81b0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical, categorical, and y columns\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']\n",
    "categorical_cols = ['team_direction', 'role']\n",
    "y_cols = ['x_future', 'y_future']\n",
    "\n",
    "# Define parameters for model training\n",
    "n_epochs = 1\n",
    "batch_size = 32\n",
    "n_matches = 10\n",
    "downsampling_factor = 5     # Keep every n:th frame\n",
    "sequence_length = 5    # Sequence length for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define denominators for normalization\n",
    "denominators = {\n",
    "    'x': pitch_length,\n",
    "    'y': pitch_width,\n",
    "    'v_y': 13,\n",
    "    'v_y': 13,\n",
    "    'a_y': 10,\n",
    "    'a_y': 10,\n",
    "    'acc': 20,\n",
    "    'pac': 20,\n",
    "    'sta': 20,\n",
    "    'height': 2.10,\n",
    "    'weight': 110,\n",
    "    'distance_to_ball': round(np.sqrt((pitch_length**2 + pitch_width**2)), 2),\n",
    "    'angle_to_ball': 360,\n",
    "    'orientation': 360,\n",
    "    'tiredness': 10,\n",
    "    'minute': 45,\n",
    "    'period': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f6979",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the DataFrame before training\n",
    "def prepare_df(frames_df, positions=None, include_ball=True, ball_has_to_be_in_motion=False):\n",
    "    # Fill NaN values with zeros for numerical columns\n",
    "    frames_df[numerical_cols] = frames_df[numerical_cols].fillna(0)\n",
    "\n",
    "    # Drop rows with NaN values in the labels (y)\n",
    "    frames_df.dropna(subset=y_cols, inplace=True)\n",
    "\n",
    "    # Drop rows where 'team' is ball, if specified\n",
    "    if not include_ball:\n",
    "        frames_df = frames_df[frames_df['team'] != 'ball']\n",
    "\n",
    "    # Drop rows where ball is not in motion, if specified\n",
    "    if ball_has_to_be_in_motion:\n",
    "        frames_df = frames_df[frames_df['ball_in_motion']]\n",
    "\n",
    "    # # Drop rows where all objects werent detected\n",
    "    # frames_df = frames_df[frames_df['objects_tracked'] == 23]\n",
    "\n",
    "    # Only keep ever n:th frame\n",
    "    frames_df = frames_df[frames_df['frame'] % downsampling_factor == 0]\n",
    "\n",
    "    # Only keep goalkeeper frames\n",
    "    if positions:\n",
    "        frames_df = frames_df[frames_df['position'].isin(positions)]\n",
    "\n",
    "    return frames_df\n",
    "\n",
    "# Prepare data before training\n",
    "def prepare_data(frames_dfs, numerical_cols=numerical_cols, categorical_cols=categorical_cols, unchanged_cols=[], positions=None, include_ball=True, ball_has_to_be_in_motion=False):\n",
    "    # Initialize lists to store features and labels\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # For each game\n",
    "    for frames_df in frames_dfs:\n",
    "        # Prepare the DataFrame\n",
    "        frames_df = prepare_df(frames_df, positions=positions, include_ball=True, ball_has_to_be_in_motion=False)\n",
    "\n",
    "        # Extract features and labels from group\n",
    "        X = frames_df[numerical_cols + categorical_cols + unchanged_cols]\n",
    "        y = frames_df[y_cols]\n",
    "\n",
    "        # Append the data\n",
    "        X_data.append(X)\n",
    "        y_data.append(y)\n",
    "\n",
    "    # Concatenate the lists to create the final feature and label DataFrame\n",
    "    X_data_df = pd.concat(X_data)\n",
    "    y_data_df = pd.concat(y_data)\n",
    "\n",
    "    # Apply label encoding to categorical variables\n",
    "    for col in categorical_cols:\n",
    "        label_encoder = LabelEncoder()\n",
    "        X_data_df[col] = label_encoder.fit_transform(X_data_df[col])\n",
    "\n",
    "    # Apply custom normalization\n",
    "    for col in numerical_cols:\n",
    "        if col in denominators:\n",
    "            X_data_df[col] = X_data_df[col] / denominators[col]\n",
    "\n",
    "    # Convert categorical columns to int\n",
    "    X_data_df[categorical_cols] = X_data_df[categorical_cols].astype('int8')\n",
    "\n",
    "    return X_data_df, y_data_df\n",
    "\n",
    "# TODO: Prepare sequential data for the LSTM model. Remeber to group by ['team', 'jersey_number', 'match_id']\n",
    "def prepare_sequential_data(X_data_df, y_data_df, sequence_length):\n",
    "    # Convert pandas DataFrames to NumPy arrays\n",
    "    X_data_np = X_data.to_numpy()\n",
    "    y_data_np = y_data.to_numpy()\n",
    "\n",
    "    data_length = len(X_data)\n",
    "\n",
    "    # Create an array of indices to extract sequences\n",
    "    indices = np.arange(data_length - sequence_length + 1)[:, None] + np.arange(sequence_length)\n",
    "\n",
    "    # Use advanced indexing to extract sequences directly\n",
    "    X_seq = X_data_np[indices]\n",
    "    y_seq = y_data_np[sequence_length - 1:]\n",
    "\n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1288175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for distance wrongly predicted (in metres) for each object\n",
    "def add_pred_error(frames_df):\n",
    "    # Create a vector with the Eculidian distance between the true position and the predicted position\n",
    "    frames_df['pred_error'] = round(((frames_df['x_future_pred'] - frames_df['x_future'])**2 + (frames_df['y_future_pred'] - frames_df['y_future'])**2)**0.5, 2)\n",
    "    \n",
    "# Add a column for distance wrongly predicted (in metres) for each object. Also return average_pred_error\n",
    "def total_error_loss(frames_df, include_ball=False, ball_has_to_be_in_motion=True):\n",
    "    # Add 'pred_error' column\n",
    "    add_pred_error(frames_df)\n",
    "    \n",
    "    # Create a new column to store modified pred_error values\n",
    "    frames_df['pred_error_tmp'] = frames_df['pred_error']\n",
    "    \n",
    "    # If specified, set pred_error to None for frames where the ball is not in motion\n",
    "    if ball_has_to_be_in_motion:\n",
    "        frames_df.loc[frames_df[\"ball_in_motion\"] != True, 'pred_error_tmp'] = None\n",
    "\n",
    "    # If specified, set pred_error to None for rows where 'team' is 'ball'\n",
    "    if not include_ball:\n",
    "        frames_df.loc[frames_df['team'] == 'ball', 'pred_error_tmp'] = None\n",
    "\n",
    "    # Calculate average pred_error_tmp, excluding rows where pred_error is None\n",
    "    average_pred_error = frames_df['pred_error_tmp'].mean()\n",
    "\n",
    "    # Drop the temporary column\n",
    "    frames_df.drop(columns=['pred_error_tmp'], inplace=True)\n",
    "\n",
    "    return round(average_pred_error, 3)\n",
    "\n",
    "# Smooth the vectors 'x_future_pred' and 'y_future_pred'\n",
    "def smooth_predictions_xy(frames_df, alpha=0.93):\n",
    "    # Group by unique combinations of 'team', 'jersey_number', and 'match_id'\n",
    "    grouped = frames_df.groupby(['team', 'jersey_number', 'match_id'])\n",
    "    \n",
    "    # Apply the Exponential Moving Average filter to smooth the predictions\n",
    "    def apply_ema(x):\n",
    "        return x.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    frames_df['x_future_pred'] = grouped['x_future_pred'].transform(apply_ema)\n",
    "    frames_df['y_future_pred'] = grouped['y_future_pred'].transform(apply_ema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef43b7",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load every frames_df to a list\n",
    "frames_dfs = load_processed_frames(n_matches=n_matches)\n",
    "\n",
    "# Create an internal match_id for each game\n",
    "match_ids = range(len(frames_dfs))\n",
    "\n",
    "# Split match IDs into train, test, and validation sets\n",
    "train_ids, test_ids, val_ids = split_match_ids(match_ids=match_ids)\n",
    "\n",
    "# Select frames data for training, testing, and validation\n",
    "train_frames_dfs = [frames_dfs[i] for i in train_ids]\n",
    "test_frames_dfs = [frames_dfs[i] for i in test_ids]\n",
    "val_frames_dfs = [frames_dfs[i] for i in val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8af494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NAIVE: Always predict that all players will continue with the same velocity and acceleration\n",
    "# # The calculations are based on x, y, v_x, v_y, a_x, and a_y\n",
    "# def predict_two_seconds_naive_acceleration(frames_df):\n",
    "#     # Calculate future positions using kinematic equations\n",
    "#     frames_df['x_future_naive_acc'] = frames_df['x'] + frames_df['v_x'] * seconds_into_the_future + 0.5 * frames_df['a_x'] * (seconds_into_the_future ** 2)\n",
    "#     frames_df['y_future_naive_acc'] = frames_df['y'] + frames_df['v_y'] * seconds_into_the_future + 0.5 * frames_df['a_y'] * (seconds_into_the_future ** 2)\n",
    "\n",
    "# for train_frames_df in train_frames_dfs:\n",
    "#     predict_two_seconds_naive_acceleration(train_frames_df)\n",
    "\n",
    "# for val_frames_df in val_frames_dfs:\n",
    "#     predict_two_seconds_naive_acceleration(val_frames_df)\n",
    "\n",
    "# for test_frames_df in test_frames_dfs:\n",
    "#     predict_two_seconds_naive_acceleration(test_frames_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### NN with Embedding layers\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e649163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding configuration for each categorical column\n",
    "embedding_config = {\n",
    "    'team_direction': {'n_categories': 2, 'output_dim': 2},\n",
    "    'role': {'n_categories': 13, 'output_dim': 6},\n",
    "    'position': {'n_categories': 10, 'output_dim': 5},\n",
    "    'nationality': {'n_categories': 20, 'output_dim': 7}\n",
    "}\n",
    "\n",
    "# Create an embedding layer for a specific categorical feature.\n",
    "def create_embedding_layer(name, n_categories, output_dim):\n",
    "    input_layer = Input(shape=(1,), name=f\"{name}_input\")  # Input for feature\n",
    "    embedding = Embedding(input_dim=n_categories, output_dim=output_dim, name=f\"{name}_embedding\")(input_layer)  # Embedding\n",
    "    flat_layer = Flatten()(embedding)  # Flatten the output for dense layer\n",
    "    return input_layer, flat_layer\n",
    "\n",
    "# Generate embedding layers for all categorical features specified.\n",
    "def create_embeddings(categorical_cols):\n",
    "    input_layers = []  # Stores input layers\n",
    "    flat_layers = []  # Stores flattened outputs\n",
    "    \n",
    "    # Process each categorical column\n",
    "    for col in categorical_cols:\n",
    "        config = embedding_config.get(col)  # Get config\n",
    "        if config:\n",
    "            n_categories = config['n_categories']\n",
    "            output_dim = config['output_dim']\n",
    "            input_layer, flat_layer = create_embedding_layer(col, n_categories, output_dim)\n",
    "            input_layers.append(input_layer)\n",
    "            flat_layers.append(flat_layer)\n",
    "    \n",
    "    return input_layers, flat_layers\n",
    "\n",
    "# Choose the appropriate regularizer based on l1 and l2 values.\n",
    "def define_regularizers(l1=0, l2=0):\n",
    "    if l1 != 0:\n",
    "        return regularizers.l1(l1)\n",
    "    elif l2 != 0:\n",
    "        return regularizers.l2(l2)\n",
    "    return None\n",
    "\n",
    "# Adjust the X_data for embedding layers\n",
    "def adjust_for_embeddings(X_data_df, categorical_cols):\n",
    "    # Split the DataFrame into numerical and categorical components\n",
    "    X_numerical = X_data_df.drop(columns=categorical_cols)\n",
    "    \n",
    "    # Extract and convert categorical columns to a list of arrays if categorical_cols is not empty\n",
    "    X_categorical = [X_data_df[col].values.reshape(-1, 1) for col in categorical_cols] if categorical_cols else []\n",
    "    \n",
    "    return X_numerical, X_categorical\n",
    "\n",
    "def prepare_model_inputs(X_numerical, X_categorical):\n",
    "    # Convert list of categorical arrays into a single 2D numpy array if X_categorical is not empty\n",
    "    X_categorical_concatenated = np.concatenate(X_categorical, axis=1) if X_categorical else None\n",
    "    \n",
    "    # Combine numerical and categorical arrays\n",
    "    X_input = [X_categorical_concatenated, X_numerical] if X_categorical else [X_numerical]\n",
    "    \n",
    "    return X_input\n",
    "\n",
    "def prepare_EL_input_data(frames_dfs, numerical_cols, categorical_cols, positions=None):\n",
    "    # Prepare data\n",
    "    X, y = prepare_data(frames_dfs, numerical_cols=numerical_cols, categorical_cols=categorical_cols, positions=positions, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "\n",
    "    # No need to do anything more if 'categorical_cols' is empty\n",
    "    if categorical_cols == []:\n",
    "        return X, y\n",
    "\n",
    "    # Adjust for embeddings\n",
    "    X_numerical, X_categorical = adjust_for_embeddings(X, categorical_cols)\n",
    "\n",
    "    # Prepare inputs\n",
    "    X_input = prepare_model_inputs(X_numerical, X_categorical)\n",
    "\n",
    "    return X_input, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005fb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model with embeddings for categorical features.\n",
    "def define_NN_model_with_embedding(numerical_input_shape, l1=0, l2=0):\n",
    "    if categorical_cols:\n",
    "        categorical_inputs, categorical_flats = create_embeddings(categorical_cols)  # Create embeddings\n",
    "        numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')  # Numerical input\n",
    "        concatenated_features = Concatenate()([*categorical_flats, numerical_input])  # Combine all features\n",
    "        model_inputs = [*categorical_inputs, numerical_input]  # Model inputs\n",
    "    else:\n",
    "        numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')  # Numerical input\n",
    "        concatenated_features = numerical_input  # Use only numerical input\n",
    "        model_inputs = numerical_input  # Model inputs\n",
    "    \n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "\n",
    "    output_layer = Dense(2)(dense_layer_2)  # Output layer for x_future and y_future\n",
    "    model = Model(inputs=model_inputs, outputs=output_layer)  # Build model\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, positions=None, l1=0, l2=0, special_text=None):\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_EL_input_data(train_frames_dfs, numerical_cols, categorical_cols, positions=positions)\n",
    "    X_val_input, y_val = prepare_EL_input_data(val_frames_dfs, numerical_cols, categorical_cols, positions=positions)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model_with_embedding(numerical_input_shape=len(numerical_cols), l1=l1, l2=l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        f.write(f\"positions={positions}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the NN model with embedding layers\n",
    "# n_epochs = 5\n",
    "# categorical_cols = ['position']\n",
    "# positions = [\"Attacking Midfielder\", \"Central Midfielder\", \"Centre-Back\", \"Defensive Midfielder\", \"Forward\", \"Full-Back\", \"Goalkeeper\", \"Wide Midfielder\", \"Winger\"]\n",
    "# # positions = [\"Goalkeeper\"]\n",
    "\n",
    "# numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball']\n",
    "# train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, positions=positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training session tonight\n",
    "# # n_matches = 120\n",
    "n_epochs = 5\n",
    "categorical_cols = ['position']\n",
    "positions = [\"Attacking Midfielder\", \"Central Midfielder\", \"Centre-Back\", \"Defensive Midfielder\", \"Forward\", \"Full-Back\", \"Goalkeeper\", \"Wide Midfielder\", \"Winger\"]\n",
    "\n",
    "numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'height']\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, positions=positions)\n",
    "\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball']\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, positions=positions)\n",
    "\n",
    "numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'height']\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, positions=positions, l2=1e-05)\n",
    "\n",
    "categorical_cols = ['position', 'nationality']\n",
    "numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'height']\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, positions=positions)\n",
    "\n",
    "categorical_cols = ['role']\n",
    "positions = [\"Attacking Midfielder\", \"Central Midfielder\", \"Centre-Back\", \"Defensive Midfielder\", \"Forward\", \"Full-Back\", \"Goalkeeper\", \"Wide Midfielder\", \"Winger\"]\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, positions=positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tf model\n",
    "def load_model(model_path, euclidean_distance_loss=False):\n",
    "    try:\n",
    "        # Load the model using Keras's load_model function\n",
    "        if euclidean_distance_loss:\n",
    "            # Define custom_objects dictionary with the custom loss function\n",
    "            custom_objects = {'euclidean_distance_loss': euclidean_distance_loss}\n",
    "            return keras_load_model(model_path, custom_objects=custom_objects) \n",
    "        else:\n",
    "            return keras_load_model(model_path)\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# Extract variable from model txt file\n",
    "def extract_variables(model_name):\n",
    "    # Define the file path\n",
    "    file_path = f\"models/{model_name}.txt\"\n",
    "\n",
    "    # Initialize variables to store variables\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "    positions = []\n",
    "\n",
    "    # Read the file line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Check if the line contains the '=' character\n",
    "            if '=' in line:\n",
    "                # Split each line based on '=' and strip whitespace\n",
    "                key, value = map(str.strip, line.split('='))\n",
    "                # Check if the line corresponds to numerical_cols or categorical_cols\n",
    "                if key == 'numerical_cols':\n",
    "                    numerical_cols = eval(value)  # Convert string representation to list\n",
    "                elif key == 'categorical_cols':\n",
    "                    categorical_cols = eval(value)  # Convert string representation to list\n",
    "                elif key == 'positions':\n",
    "                    positions = eval(value)  # Convert string representation to list\n",
    "\n",
    "    return numerical_cols, categorical_cols, positions\n",
    "\n",
    "# Example usage: evaluate_model(\"NN_embedding_model_11\") \n",
    "def evaluate_model(model_name):\n",
    "    # Load varibles\n",
    "    numerical_cols, categorical_cols, positions = extract_variables(model_name)\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(f\"models/{model_name}.h5\", euclidean_distance_loss=True)\n",
    "\n",
    "    # Prepare the input data and the DataFrame itself\n",
    "    X_test_input, y_test = prepare_EL_input_data(test_frames_dfs, numerical_cols, categorical_cols, positions)\n",
    "    test_prepared_frames_dfs = [prepare_df(test_frames_df, positions=positions) for test_frames_df in test_frames_dfs]\n",
    "\n",
    "    # Concatenate the frames DataFrames into a single large DataFrame\n",
    "    frames_concatenated_df = pd.concat(test_prepared_frames_dfs, ignore_index=True)\n",
    "\n",
    "    # Make predictions using the loaded tf model\n",
    "    predictions = model.predict(X_test_input)\n",
    "\n",
    "    # Extract the predicted values\n",
    "    x_future_pred = predictions[:, 0]\n",
    "    y_future_pred = predictions[:, 1]\n",
    "\n",
    "    # Add the predicted values to 'frames_concatenated_df'\n",
    "    frames_concatenated_df['x_future_pred'] = x_future_pred\n",
    "    frames_concatenated_df['y_future_pred'] = y_future_pred\n",
    "\n",
    "    # Clip values to stay on the pitch\n",
    "    frames_concatenated_df['x_future_pred'] = frames_concatenated_df['x_future_pred'].clip(lower=0, upper=pitch_length)\n",
    "    frames_concatenated_df['y_future_pred'] = frames_concatenated_df['y_future_pred'].clip(lower=0, upper=pitch_width)\n",
    "\n",
    "    # Smooth the predicted coordinates\n",
    "    # smooth_predictions_xy(frames_df, alpha=0.98)\n",
    "\n",
    "    error = total_error_loss(frames_concatenated_df)\n",
    "\n",
    "    return error\n",
    "\n",
    "# model_names = [\"NN_model_v2\"]\n",
    "# for model_name in model_names:\n",
    "#     error = evaluate_model(model_name)\n",
    "#     print(f\"{model_name}: {error}\")\n",
    "\n",
    "# for i in range(2, 12):\n",
    "#     model_name = f\"NN_model_v{i}\"\n",
    "#     error = evaluate_model(model_name)\n",
    "#     print(f\"{model_name}: {error}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfb31f",
   "metadata": {},
   "source": [
    "### Test for different alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: run_model(\"NN_embedding_model_11\") \n",
    "def run_model(model_name):\n",
    "    # Load varibles\n",
    "    numerical_cols, categorical_cols, positions = extract_variables(model_name)\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(f\"models/{model_name}.h5\", euclidean_distance_loss=True)\n",
    "\n",
    "    # Prepare the input data and the DataFrame itself\n",
    "    X_test_input, y_test = prepare_EL_input_data(test_frames_dfs, numerical_cols, categorical_cols, positions)\n",
    "    test_prepared_frames_dfs = [prepare_df(test_frames_df, positions=positions) for test_frames_df in test_frames_dfs]\n",
    "\n",
    "    # Concatenate the frames DataFrames into a single large DataFrame\n",
    "    frames_concatenated_df = pd.concat(test_prepared_frames_dfs, ignore_index=True)\n",
    "\n",
    "    # Make predictions using the loaded tf model\n",
    "    predictions = model.predict(X_test_input)\n",
    "\n",
    "    # Extract the predicted values\n",
    "    x_future_pred = predictions[:, 0]\n",
    "    y_future_pred = predictions[:, 1]\n",
    "\n",
    "    # Add the predicted values to 'frames_concatenated_df'\n",
    "    frames_concatenated_df['x_future_pred'] = x_future_pred\n",
    "    frames_concatenated_df['y_future_pred'] = y_future_pred\n",
    "\n",
    "    # Clip values to stay on the pitch\n",
    "    frames_concatenated_df['x_future_pred'] = frames_concatenated_df['x_future_pred'].clip(lower=0, upper=pitch_length)\n",
    "    frames_concatenated_df['y_future_pred'] = frames_concatenated_df['y_future_pred'].clip(lower=0, upper=pitch_width)\n",
    "\n",
    "    return frames_concatenated_df\n",
    "\n",
    "# frames_df = run_model(\"NN_best_v1\")\n",
    "    \n",
    "# alpha = 1.0\n",
    "# for i in range(10):\n",
    "#     frames_df = frames_df.copy()\n",
    "#     smooth_predictions_xy(frames_df, alpha=alpha)\n",
    "#     error = total_error_loss(frames_df)\n",
    "#     print(f\"{round(alpha, 2)}: {error}\")\n",
    "#     alpha -= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates how the average 'pred_error' varies with each value in 'column_to_analyze'\n",
    "def find_column_variance(frames_df, column_to_analyze):\n",
    "    # Convert 'pred_error' to numeric, coercing non-numeric values to NaN\n",
    "    frames_df['pred_error'] = pd.to_numeric(frames_df['pred_error'], errors='coerce')\n",
    "\n",
    "    # Group by 'column_to_analyze' and calculate the average 'pred_error'\n",
    "    column_variance_df = frames_df.groupby(column_to_analyze)['pred_error'].mean().reset_index()\n",
    "\n",
    "    # Round to 2 decimal places\n",
    "    column_variance_df['pred_error'] = round(column_variance_df['pred_error'], 2)\n",
    "\n",
    "    # Sort by 'column_to_analyze' in ascending order\n",
    "    column_variance_df = column_variance_df.sort_values(by=column_to_analyze, ascending=True)\n",
    "\n",
    "    # Return DataFrame with results\n",
    "    return column_variance_df\n",
    "\n",
    "# alpha = 1\n",
    "# # frames_df = run_model(\"best\")\n",
    "# total_error_loss(frames_df)\n",
    "# column_to_analyze = 'nationality'\n",
    "\n",
    "# # Call the function\n",
    "# column_variance_df = find_column_variance(frames_df, column_to_analyze)\n",
    "\n",
    "# # Print the DataFrame with results\n",
    "# print(f\"Average Pred Error per {column_to_analyze}:\")\n",
    "# print(column_variance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ccb12",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### LSTM model\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LSTM_model(input_shape):\n",
    "    # Define the lengt of the sequence\n",
    "    timesteps = 5 * FPS\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, activation='relu', input_shape=(timesteps, input_shape[1])),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(X_train, y_train, X_val, y_val, val_frames_dfs):\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(X_train.shape[1:])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model.save(get_next_model_filename(\"LSTM_model\"))\n",
    "\n",
    "    # Print the error using total_error_loss function\n",
    "    train_error = total_error_loss(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    val_error = total_error_loss(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    print(\"Training Error:\", train_error)\n",
    "    print(\"Validation Error:\", val_error)\n",
    "\n",
    "def train_LSTM(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare the data for training\n",
    "    X_train, y_train = prepare_data(train_frames_dfs)\n",
    "\n",
    "    # Prepare the data for validation\n",
    "    X_val, y_val = prepare_data(val_frames_dfs)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(X_train, y_train, X_val, y_val, val_frames_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding configuration for each categorical column\n",
    "embedding_config = {\n",
    "    'team_direction': {'n_categories': 2, 'output_dim': 2},\n",
    "    'role': {'n_categories': 13, 'output_dim': 6},\n",
    "    'position': {'n_categories': 10, 'output_dim': 5},\n",
    "    'nationality': {'n_categories': 20, 'output_dim': 7}\n",
    "}\n",
    "\n",
    "# Create an embedding layer for a specific categorical feature.\n",
    "def create_embedding_layer(name, n_categories, output_dim):\n",
    "    input_layer = Input(shape=(1,), name=f\"{name}_input\")  # Input for feature\n",
    "    embedding = Embedding(input_dim=n_categories, output_dim=output_dim, name=f\"{name}_embedding\")(input_layer)  # Embedding\n",
    "    flat_layer = Flatten()(embedding)  # Flatten the output for dense layer\n",
    "    return input_layer, flat_layer\n",
    "\n",
    "# Generate embedding layers for all categorical features specified.\n",
    "def create_embeddings(categorical_cols):\n",
    "    input_layers = []  # Stores input layers\n",
    "    flat_layers = []  # Stores flattened outputs\n",
    "    \n",
    "    # Process each categorical column\n",
    "    for col in categorical_cols:\n",
    "        config = embedding_config.get(col)  # Get config\n",
    "        if config:\n",
    "            n_categories = config['n_categories']\n",
    "            output_dim = config['output_dim']\n",
    "            input_layer, flat_layer = create_embedding_layer(col, n_categories, output_dim)\n",
    "            input_layers.append(input_layer)\n",
    "            flat_layers.append(flat_layer)\n",
    "    \n",
    "    return input_layers, flat_layers\n",
    "\n",
    "# Choose the appropriate regularizer based on l1 and l2 values.\n",
    "def define_regularizers(l1=0, l2=0):\n",
    "    if l1 != 0:\n",
    "        return regularizers.l1(l1)\n",
    "    elif l2 != 0:\n",
    "        return regularizers.l2(l2)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Adjust the X_data for embedding layers\n",
    "def adjust_for_embeddings(X_data_df, categorical_cols):\n",
    "    # Split the DataFrame into numerical and categorical components\n",
    "    X_numerical = X_data_df.drop(columns=categorical_cols)\n",
    "    \n",
    "    # Extract and convert categorical columns to a list of arrays if categorical_cols is not empty\n",
    "    X_categorical = [X_data_df[col].values.reshape(-1, 1) for col in categorical_cols] if categorical_cols else []\n",
    "    \n",
    "    return X_numerical, X_categorical\n",
    "\n",
    "def prepare_model_inputs(X_numerical, X_categorical):\n",
    "    # Convert list of categorical arrays into a single 2D numpy array if X_categorical is not empty\n",
    "    X_categorical_concatenated = np.concatenate(X_categorical, axis=1) if X_categorical else None\n",
    "    \n",
    "    # Combine numerical and categorical arrays\n",
    "    X_input = [X_categorical_concatenated, X_numerical] if X_categorical else [X_numerical]\n",
    "    \n",
    "    return X_input\n",
    "\n",
    "# Add a vector indicating if the row can be sequentialized, i.e. the player has 'sequence_length' consecutive frames\n",
    "def add_can_be_sequentialized(frames_df, sequence_length):\n",
    "    # Calculate the expected sequence start frame\n",
    "    frames_df['expected_sequence_start_frame'] = frames_df['frame'] - sequence_length * FPS // downsampling_factor\n",
    "    \n",
    "    # Group by each unique player\n",
    "    grouped = frames_df.groupby(['team', 'jersey_number'])\n",
    "    \n",
    "    # For each player, shift the 'frame' column to identify potential sequences\n",
    "    frames_df['shifted_frame'] = grouped['frame'].shift(sequence_length)\n",
    "    \n",
    "    # Check if the shifted frame matches 'expected_sequence_start_frame'\n",
    "    # and set 'can_be_sequentialized' to True if it does\n",
    "    frames_df['can_be_sequentialized'] = frames_df['expected_sequence_start_frame'] == frames_df['shifted_frame']\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    frames_df.drop(['expected_sequence_start_frame', 'shifted_frame'], axis=1, inplace=True)\n",
    "\n",
    "# Sequentialize the numerical_columns\n",
    "def sequentialize_numerical_cols(X_df, numerical_cols, sequence_length):\n",
    "    # Initialize an empty DataFrame to store the sequentialized data\n",
    "    sequentialized_data = []\n",
    "    \n",
    "    # Group by each unique player\n",
    "    grouped = X_df.groupby(['team', 'jersey_number', 'match_id'])\n",
    "    \n",
    "    # Iterate through each group and create sequences\n",
    "    for _, group in grouped:\n",
    "        # Create shifted versions of the DataFrame for each step in the sequence\n",
    "        shifted_dfs = [group[numerical_cols].shift(i) for i in range(sequence_length)]\n",
    "\n",
    "        # Concatenate the shifted DataFrames along the columns axis to form sequences\n",
    "        sequences_df = pd.concat(shifted_dfs, axis=1)\n",
    "\n",
    "        # Only consider groups that can be sequentialized\n",
    "        sequences_df = sequences_df[group['can_be_sequentialized']]\n",
    "\n",
    "        X_df['']\n",
    "\n",
    "    #     # Add to the list of sequentialized data\n",
    "    #     sequentialized_data.append(sequences_df)\n",
    "    \n",
    "    # # Combine all the sequentialized data\n",
    "    # X_sequentialized_df = pd.concat(sequentialized_data)\n",
    "    \n",
    "    return X_sequentialized_df\n",
    "\n",
    "def prepare_LSTM_input_data(frames_dfs, numerical_cols, categorical_cols, sequence_length, positions=positions):\n",
    "    # Definie columns temporarely give to prepare_data()\n",
    "    unchanged_cols=['player', 'frame', 'team', 'jersey_number', 'match_id']\n",
    "\n",
    "    # Prepare data\n",
    "    X_df, y_df = prepare_data(frames_dfs, numerical_cols=numerical_cols, categorical_cols=categorical_cols, unchanged_cols=unchanged_cols, positions=positions, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "\n",
    "    # Add the vector that determines if the vector can be sequentialized\n",
    "    add_can_be_sequentialized(X_df, sequence_length=sequence_length)\n",
    "    \n",
    "    # Trim y_df based on 'can_be_sequentialized'\n",
    "    y_df = y_df[X_df['can_be_sequentialized']]\n",
    "\n",
    "    # Sequentialize the numerical_cols\n",
    "    X_sequentialized_df = sequentialize_numerical_cols(X_df, numerical_cols, sequence_length)\n",
    "\n",
    "    # No need to do anything more if 'categorical_cols' is empty\n",
    "    if categorical_cols == []:\n",
    "        return X_sequentialized_df, y_df\n",
    "\n",
    "    # Adjust for embeddings\n",
    "    _, X_categorical = adjust_for_embeddings(X_df, categorical_cols)\n",
    "\n",
    "    # Prepare inputs\n",
    "    X_input = prepare_model_inputs(X_numerical, X_categorical)\n",
    "\n",
    "    return X_input, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NN model with LSTM layer\n",
    "def define_LSTM_model(numerical_input_shape, l1=0, l2=0):\n",
    "    if categorical_cols:\n",
    "        categorical_inputs, categorical_flats = create_embeddings(categorical_cols)  # Create embeddings\n",
    "        numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')  # Numerical input\n",
    "        concatenated_features = Concatenate()([*categorical_flats, numerical_input])  # Combine all features\n",
    "        model_inputs = [*categorical_inputs, numerical_input]  # Model inputs\n",
    "    else:\n",
    "        numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')  # Numerical input\n",
    "        concatenated_features = numerical_input  # Use only numerical input\n",
    "        model_inputs = numerical_input  # Model inputs\n",
    "    \n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "\n",
    "    output_layer = Dense(2)(dense_layer_2)  # Output layer for x_future and y_future\n",
    "    model = Model(inputs=model_inputs, outputs=output_layer)  # Build model\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(train_frames_dfs, val_frames_dfs, sequence_length, positions=None, l1=0, l2=0, special_text=None):\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_LSTM_input_data(train_frames_dfs, numerical_cols, categorical_cols, sequence_length. positions=positions)\n",
    "    X_val_input, y_val = prepare_LSTM_input_data(val_frames_dfs, numerical_cols, categorical_cols, sequence_length, positions=positions)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(numerical_input_shape=len(numerical_cols), l1=l1, l2=l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"LSTM_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"sequence_length={sequence_length}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        f.write(f\"positions={positions}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5140068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 1\n",
    "# categorical_cols = []\n",
    "# positions = [\"Goalkeeper\"]\n",
    "# sequence_length = 3\n",
    "\n",
    "# numerical_cols=['x', 'y']\n",
    "# X, y = prepare_LSTM_input_data(test_frames_dfs, numerical_cols, categorical_cols, sequence_length, positions)\n",
    "# # train_LSTM_model(train_frames_dfs, val_frames_dfs, positions=positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d70d0",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize training results\n",
    "# model_name = 'NN_embedding_model_3'\n",
    "# training_results = {\n",
    "#     'loss': [2.0478146076202393, 2.0088889598846436, 2.0007753372192383, 1.9968146085739136, 1.9937269687652588, 1.9921172857284546, 1.990675687789917, 1.9893001317977905, 1.9881930351257324, 1.9875684976577759, 1.9872304201126099, 1.9865171909332275, 1.9859004020690918, 1.985435128211975, 1.9848004579544067, 1.983401894569397, 1.9824390411376953, 1.9820188283920288, 1.981824517250061, 1.9817743301391602],\n",
    "#     'val_loss': [4.535243034362793, 4.51762580871582, 4.469428539276123, 4.436275482177734, 4.456634521484375, 4.815524578094482, 4.3103556632995605, 4.498797416687012, 4.790141582489014, 4.464589595794678, 4.674554347991943, 4.561259746551514, 4.533383369445801, 4.472135066986084, 4.466953754425049, 4.478504180908203, 4.723540782928467, 4.859069347381592, 4.496937274932861, 4.377903461456299]\n",
    "# }\n",
    "\n",
    "# visualize_training_results(training_results, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
