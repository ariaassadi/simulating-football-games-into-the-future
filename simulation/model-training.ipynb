{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "In this notebook, we have all the functions used for training the deep learning models. We use leverage two architechtures with one NN model and one LSTM model.\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 06:54:51.968400: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, BatchNormalization, Dropout, Reshape, LSTM\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import date\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from utils import load_processed_frames, split_match_ids, embedding_config, get_next_model_filename, euclidean_distance_loss, total_error_loss, define_regularizers, prepare_EL_input_data, prepare_LSTM_input_data, create_embeddings, smooth_predictions_xy, run_model, evaluate_model, print_column_variance\n",
    "from utils import prepare_df, add_can_be_sequentialized, extract_variables, load_tf_model, prepare_LSTM_df\n",
    "from visualize_game import visualize_training_results\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf81b0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfca7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for model training\n",
    "batch_size = 32\n",
    "sequence_length = 10        # Sequence length for LSTM model\n",
    "downsampling_factor = 5     # Keep every n:th frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### NN with Embedding layers\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005fb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the neural network model with embeddings layers\n",
    "def define_NN_model(numerical_input_shape, categorical_cols, l1=0, l2=0):\n",
    "    # Inputs for each categorical feature\n",
    "    categorical_inputs = []\n",
    "    categorical_flats = []\n",
    "    for col in categorical_cols:\n",
    "        # Replace spaces with underscores in the input name\n",
    "        input_name = f'input_{col.replace(\" \", \"_\")}'\n",
    "        embedding_name = f'embedding_{col.replace(\" \", \"_\")}'\n",
    "\n",
    "        cat_input = Input(shape=(1,), name=input_name)  # Input for each categorical feature\n",
    "        emb_layer = Embedding(\n",
    "            input_dim=embedding_config[col]['n_categories'],\n",
    "            output_dim=embedding_config[col]['output_dim'],\n",
    "            input_length=1,\n",
    "            name=embedding_name\n",
    "        )(cat_input)\n",
    "        flat_layer = Flatten()(emb_layer)\n",
    "        categorical_inputs.append(cat_input)\n",
    "        categorical_flats.append(flat_layer)\n",
    "\n",
    "    # Prepare input layer for numerical data\n",
    "    numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')\n",
    "\n",
    "    # Concatenate all flattened embeddings with the numerical input\n",
    "    concatenated_features = Concatenate()([*categorical_flats, numerical_input]) if categorical_flats else numerical_input\n",
    "\n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "    output_layer = Dense(len(y_cols), name='output_layer')(dense_layer_2)  # Output layer 'x_future' and 'y_future'\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[*categorical_inputs, numerical_input], outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_NN_model(train_ids, val_ids, numerical_cols, categorical_cols, positions, l1=0, l2=0, special_text=None):\n",
    "    # Start time to later display how many seconds the execution too\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_EL_input_data(train_ids, numerical_cols, categorical_cols, positions, downsampling_factor)\n",
    "    X_val_input, y_val = prepare_EL_input_data(val_ids, numerical_cols, categorical_cols, positions, downsampling_factor)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model(len(numerical_cols), categorical_cols, l1, l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        f.write(f\"positions={positions}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the execution time\n",
    "        end_time = time.time()\n",
    "        execution_time_minutes = (end_time - start_time) / 60\n",
    "        f.write(f\"\\nExecution time: {execution_time_minutes:.0f} minutes\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "078b2f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 07:00:26.167020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79266 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2024-05-28 07:00:30.964448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-05-28 07:00:30.966278: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x152593ee48f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-28 07:00:30.966297: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-05-28 07:00:30.971074: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-28 07:00:31.079972: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138840/1138840 - 1670s - loss: 1.7299 - val_loss: 1.7091 - 1670s/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train the NN model with embedding layers\n",
    "n_epochs = 3\n",
    "n_matches = 560\n",
    "categorical_cols = ['position']\n",
    "positions=['Attacking Midfielder', 'Central Midfielder', 'Centre-Back', 'Defensive Midfielder', 'Forward', 'Full-Back', 'Goalkeeper', 'Wide Midfielder', 'Winger']\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'v_x_avg', 'v_y_avg']\n",
    "\n",
    "train_ids, _, val_ids = split_match_ids(n_matches)\n",
    "\n",
    "train_NN_model(train_ids, val_ids, numerical_cols, categorical_cols, positions, l2=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ccb12",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### LSTM model\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46e9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the LSTM model with embeddings layers\n",
    "def define_LSTM_model(numerical_input_shape, categorical_cols, sequence_length, l1=0, l2=0):\n",
    "    categorical_inputs = []\n",
    "    categorical_flats = []\n",
    "    \n",
    "    # Create inputs for each categorical feature\n",
    "    for col in categorical_cols:\n",
    "        input_name = f'input_{col.replace(\" \", \"_\")}'\n",
    "        embedding_name = f'embedding_{col.replace(\" \", \"_\")}'\n",
    "\n",
    "        cat_input = Input(shape=(1,), name=input_name)\n",
    "        emb_layer = Embedding(\n",
    "            input_dim=embedding_config[col]['n_categories'],\n",
    "            output_dim=embedding_config[col]['output_dim'],\n",
    "            input_length=1,\n",
    "            name=embedding_name\n",
    "        )(cat_input)\n",
    "        flat_layer = Flatten()(emb_layer)\n",
    "        categorical_inputs.append(cat_input)\n",
    "        categorical_flats.append(flat_layer)\n",
    "\n",
    "    # Prepare input layer for sequential numerical data\n",
    "    numerical_input = Input(shape=(sequence_length, numerical_input_shape), name='numerical_input')\n",
    "    lstm_layer = LSTM(64, return_sequences=False, name='lstm_numerical')(numerical_input)\n",
    "\n",
    "    # Concatenate embeddings with numerical input\n",
    "    if categorical_flats:\n",
    "        concatenated_features = Concatenate()([*categorical_flats, lstm_layer])\n",
    "    else:\n",
    "        concatenated_features = lstm_layer  # Only use LSTM output if no categorical data\n",
    "\n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "    output_layer = Dense(len(y_cols), name='output_layer')(dense_layer_2)  # Output layer\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[*categorical_inputs, numerical_input], outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions, l1=0, l2=0, special_text=None):\n",
    "    # Prepare inputs\n",
    "    start_time = time.time()\n",
    "    X_train_input, y_train = prepare_LSTM_input_data(train_ids, numerical_cols, categorical_cols, sequence_length, positions, downsampling_factor)\n",
    "    X_val_input, y_val = prepare_LSTM_input_data(val_ids, numerical_cols, categorical_cols, sequence_length, positions, downsampling_factor)\n",
    "    data_preparation_time = time.time() - start_time\n",
    "    print(f\"Data preparation time: {data_preparation_time:.2f} seconds\")\n",
    "\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(len(numerical_cols), categorical_cols, sequence_length, l1, l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"LSTM_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"sequence_length={sequence_length}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        f.write(f\"positions={positions}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the execution time\n",
    "        end_time = time.time()\n",
    "        execution_time_minutes = (end_time - start_time) / 60\n",
    "        f.write(f\"\\nExecution time: {execution_time_minutes:.0f} minutes\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5140068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation time: 1270.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 07:49:41.185534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114102/1114102 - 2787s - loss: 1.6194 - val_loss: 1.6000 - 2787s/epoch - 3ms/step\n",
      "Training time: 2793.51 seconds\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "n_epochs = 3\n",
    "n_matches = 200\n",
    "sequence_length = 10\n",
    "positions = ['Attacking Midfielder', 'Central Midfielder', 'Centre-Back', 'Defensive Midfielder', 'Forward', 'Full-Back', 'Goalkeeper', 'Wide Midfielder', 'Winger']\n",
    "categorical_cols = ['position']\n",
    "numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'v_x_avg', 'v_y_avg', 'distance_to_onside']\n",
    "\n",
    "train_ids, _, val_ids = split_match_ids(n_matches)\n",
    "train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions, l2=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d70d0",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "model_name = 'NN_model_v3'\n",
    "training_results = {\n",
    "    'loss': [2.0478146076202393, 2.0088889598846436, 2.0007753372192383, 1.9968146085739136, 1.9937269687652588, 1.9921172857284546, 1.990675687789917, 1.9893001317977905, 1.9881930351257324, 1.9875684976577759, 1.9872304201126099, 1.9865171909332275, 1.9859004020690918, 1.985435128211975, 1.9848004579544067, 1.983401894569397, 1.9824390411376953, 1.9820188283920288, 1.981824517250061, 1.9817743301391602],\n",
    "    'val_loss': [4.535243034362793, 4.51762580871582, 4.469428539276123, 4.436275482177734, 4.456634521484375, 4.815524578094482, 4.3103556632995605, 4.498797416687012, 4.790141582489014, 4.464589595794678, 4.674554347991943, 4.561259746551514, 4.533383369445801, 4.472135066986084, 4.466953754425049, 4.478504180908203, 4.723540782928467, 4.859069347381592, 4.496937274932861, 4.377903461456299]\n",
    "}\n",
    "\n",
    "visualize_training_results(training_results, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Feb  1 2024, 03:10:29) [GCC 11.3.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
