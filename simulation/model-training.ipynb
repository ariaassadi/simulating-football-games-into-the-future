{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 23:49:06.183634: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, BatchNormalization, Dropout, Reshape, LSTM\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import date\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from utils import load_processed_frames, split_match_ids, embedding_config, get_next_model_filename, euclidean_distance_loss, total_error_loss, define_regularizers, prepare_EL_input_data, prepare_LSTM_input_data, create_embeddings, smooth_predictions_xy, run_model, evaluate_model, print_column_variance\n",
    "from utils import prepare_df, add_can_be_sequentialized, extract_variables, load_tf_model\n",
    "from visualize_game import visualize_training_results\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf81b0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfca7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical, categorical, and y columns\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']\n",
    "categorical_cols = ['team_direction', 'role']\n",
    "\n",
    "# Define parameters for model training\n",
    "n_epochs = 1\n",
    "batch_size = 32\n",
    "n_matches = 10\n",
    "sequence_length = 10    # Sequence length for LSTM model\n",
    "\n",
    "# Load the match_ids for each set\n",
    "train_ids, test_ids, val_ids = split_match_ids(n_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### NN with Embedding layers\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005fb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the neural network model with embeddings layers\n",
    "def define_NN_model(numerical_input_shape, categorical_cols, l1=0, l2=0):\n",
    "    # Inputs for each categorical feature\n",
    "    categorical_inputs = []\n",
    "    categorical_flats = []\n",
    "    for col in categorical_cols:\n",
    "        # Replace spaces with underscores in the input name\n",
    "        input_name = f'input_{col.replace(\" \", \"_\")}'\n",
    "        embedding_name = f'embedding_{col.replace(\" \", \"_\")}'\n",
    "\n",
    "        cat_input = Input(shape=(1,), name=input_name)  # Input for each categorical feature\n",
    "        emb_layer = Embedding(\n",
    "            input_dim=embedding_config[col]['n_categories'],\n",
    "            output_dim=embedding_config[col]['output_dim'],\n",
    "            input_length=1,\n",
    "            name=embedding_name\n",
    "        )(cat_input)\n",
    "        flat_layer = Flatten()(emb_layer)\n",
    "        categorical_inputs.append(cat_input)\n",
    "        categorical_flats.append(flat_layer)\n",
    "\n",
    "    # Prepare input layer for numerical data\n",
    "    numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')\n",
    "\n",
    "    # Concatenate all flattened embeddings with the numerical input\n",
    "    concatenated_features = Concatenate()([*categorical_flats, numerical_input]) if categorical_flats else numerical_input\n",
    "\n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "    output_layer = Dense(2, name='output_layer')(dense_layer_2)  # Output layer 'x_future' and 'y_future'\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[*categorical_inputs, numerical_input], outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_NN_model_with_embedding(train_ids, val_ids, numerical_cols, categorical_cols, positions, l1=0, l2=0, special_text=None):\n",
    "    # Start time to later display how many seconds the execution too\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_EL_input_data(train_ids, numerical_cols, categorical_cols, positions=positions)\n",
    "    X_val_input, y_val = prepare_EL_input_data(val_ids, numerical_cols, categorical_cols, positions=positions)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model(len(numerical_cols), categorical_cols, l1, l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        f.write(f\"positions={positions}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the execution time\n",
    "        end_time = time.time()\n",
    "        execution_time_minutes = (end_time - start_time) / 60\n",
    "        f.write(f\"\\nExecution time: {execution_time_minutes:.0f} minutes\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "078b2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN model with embedding layers\n",
    "n_epochs = 1\n",
    "n_matches = 40\n",
    "categorical_cols = ['position']\n",
    "positions=['Attacking Midfielder', 'Central Midfielder', 'Centre-Back', 'Defensive Midfielder', 'Forward', 'Full-Back', 'Goalkeeper', 'Wide Midfielder', 'Winger']\n",
    "# positions = ['Goalkeeper', 'Centre-Back', 'Full-Back']\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'angle_to_ball', 'tiredness', 'v_x_avg', 'v_y_avg']\n",
    "\n",
    "train_ids, _, val_ids = split_match_ids(n_matches)\n",
    "# train_NN_model_with_embedding(train_ids, val_ids, numerical_cols, categorical_cols, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb7c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN model with embedding layers\n",
    "n_epochs = 1\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'angle_to_ball', 'tiredness', 'v_x_avg', 'v_y_avg']\n",
    "categorical_cols = ['position']\n",
    "positions=['Attacking Midfielder', 'Central Midfielder', 'Centre-Back', 'Defensive Midfielder', 'Forward', 'Full-Back', 'Goalkeeper', 'Wide Midfielder', 'Winger']\n",
    "# positions = ['Goalkeeper', 'Centre-Back', 'Full-Back']\n",
    "\n",
    "n_matches = 240\n",
    "train_ids, _, val_ids = split_match_ids(n_matches)\n",
    "train_NN_model_with_embedding(train_ids, val_ids, numerical_cols, categorical_cols, positions)\n",
    "\n",
    "n_matches = 80\n",
    "train_ids, _, val_ids = split_match_ids(n_matches)\n",
    "train_NN_model_with_embedding(train_ids, val_ids, numerical_cols, categorical_cols, positions)\n",
    "\n",
    "n_matches = 120\n",
    "train_ids, _, val_ids = split_match_ids(n_matches)\n",
    "train_NN_model_with_embedding(train_ids, val_ids, numerical_cols, categorical_cols, positions)\n",
    "\n",
    "n_matches = 180\n",
    "train_ids, _, val_ids = split_match_ids(n_matches)\n",
    "train_NN_model_with_embedding(train_ids, val_ids, numerical_cols, categorical_cols, positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58786e12",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696c04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = [\"LSTM_model_v2\"]\n",
    "# for model_name in model_names:\n",
    "#     error = evaluate_model(test_frames_dfs, model_name)\n",
    "#     print(f\"{model_name}: {error}\")\n",
    "\n",
    "# Test different alpha values\n",
    "\n",
    "# frames_df = run_model(test_frames_dfs, \"LSTM_model_v5\")\n",
    "# alpha = 1.0\n",
    "# for i in range(10):\n",
    "#     frames_df = frames_df.copy()\n",
    "#     smooth_predictions_xy(frames_df, alpha=alpha)\n",
    "#     error = total_error_loss(frames_df)\n",
    "#     print(f\"{round(alpha, 2)}: {error}\")\n",
    "#     alpha -= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91f8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print column variance for position\n",
    "# column_to_analyze = 'position'\n",
    "# model_name = \"LSTM_model_v9\"\n",
    "# print_column_variance(val_frames_dfs, model_name, column_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ccb12",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### LSTM model\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c46e9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the LSTM model with embeddings layers\n",
    "def define_LSTM_model(numerical_input_shape, categorical_cols, sequence_length, l1=0, l2=0):\n",
    "    categorical_inputs = []\n",
    "    categorical_flats = []\n",
    "    \n",
    "    # Create inputs for each categorical feature\n",
    "    for col in categorical_cols:\n",
    "        input_name = f'input_{col.replace(\" \", \"_\")}'\n",
    "        embedding_name = f'embedding_{col.replace(\" \", \"_\")}'\n",
    "\n",
    "        cat_input = Input(shape=(1,), name=input_name)\n",
    "        emb_layer = Embedding(\n",
    "            input_dim=embedding_config[col]['n_categories'],\n",
    "            output_dim=embedding_config[col]['output_dim'],\n",
    "            input_length=1,\n",
    "            name=embedding_name\n",
    "        )(cat_input)\n",
    "        flat_layer = Flatten()(emb_layer)\n",
    "        categorical_inputs.append(cat_input)\n",
    "        categorical_flats.append(flat_layer)\n",
    "\n",
    "    # Prepare input layer for sequential numerical data\n",
    "    numerical_input = Input(shape=(sequence_length, numerical_input_shape), name='numerical_input')\n",
    "    lstm_layer = LSTM(64, return_sequences=False, name='lstm_numerical')(numerical_input)\n",
    "\n",
    "    # Concatenate embeddings with numerical input\n",
    "    if categorical_flats:\n",
    "        concatenated_features = Concatenate()([*categorical_flats, lstm_layer])\n",
    "    else:\n",
    "        concatenated_features = lstm_layer  # Only use LSTM output if no categorical data\n",
    "\n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "    output_layer = Dense(2, name='output_layer')(dense_layer_2)  # Output layer 'x_future' and 'y_future'\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[*categorical_inputs, numerical_input], outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions, l1=0, l2=0, special_text=None):\n",
    "    # Start time to later display how many seconds the execution too\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_LSTM_input_data(train_ids, numerical_cols, categorical_cols, sequence_length, positions=positions)\n",
    "    X_val_input, y_val = prepare_LSTM_input_data(val_ids, numerical_cols, categorical_cols, sequence_length, positions=positions)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(len(numerical_cols), categorical_cols, sequence_length, l1, l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"LSTM_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"sequence_length={sequence_length}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        f.write(f\"positions={positions}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the execution time\n",
    "        end_time = time.time()\n",
    "        execution_time_minutes = (end_time - start_time) / 60\n",
    "        f.write(f\"\\nExecution time: {execution_time_minutes:.0f} minutes\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5140068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24317/24317 - 61s - loss: 1.7624 - val_loss: 1.1315 - 61s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "n_epochs = 1\n",
    "n_matches = 80\n",
    "positions=['Attacking Midfielder', 'Central Midfielder', 'Centre-Back', 'Defensive Midfielder', 'Forward', 'Full-Back', 'Goalkeeper', 'Wide Midfielder', 'Winger']\n",
    "# positions=['Goalkeeper']\n",
    "categorical_cols = ['position']\n",
    "sequence_length = 10\n",
    "numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball']\n",
    "\n",
    "train_ids, _, val_ids = split_match_ids(n_matches)\n",
    "train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions=positions, l2=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e61943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'tiredness_short']\n",
    "# train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions=positions, l2=1e-6)\n",
    "\n",
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'v_x_avg', 'v_y_avg']\n",
    "# train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions=positions, l2=1e-6)\n",
    "\n",
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'minute', 'sta', 'distance_ran']\n",
    "# train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions=positions, l2=1e-6)\n",
    "\n",
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'angle_to_ball']\n",
    "# train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions=positions, l2=1e-6)\n",
    "\n",
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'pac', 'acc']\n",
    "# train_LSTM_model(train_ids, val_ids, numerical_cols, categorical_cols, sequence_length, positions=positions, l2=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9539beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"LSTM_model_v7\"\n",
    "# y_test, predictions = run_model_tmp(test_ids, model_name)\n",
    "# error = tf.reduce_mean(euclidean_distance_loss(y_test, predictions)).numpy()\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad0032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"LSTM_model_v8\"\n",
    "# error = evaluate_model(test_ids, model_name)\n",
    "# print(f\"{model_name}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d70d0",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1730e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize training results\n",
    "# model_name = 'NN_embedding_model_3'\n",
    "# training_results = {\n",
    "#     'loss': [2.0478146076202393, 2.0088889598846436, 2.0007753372192383, 1.9968146085739136, 1.9937269687652588, 1.9921172857284546, 1.990675687789917, 1.9893001317977905, 1.9881930351257324, 1.9875684976577759, 1.9872304201126099, 1.9865171909332275, 1.9859004020690918, 1.985435128211975, 1.9848004579544067, 1.983401894569397, 1.9824390411376953, 1.9820188283920288, 1.981824517250061, 1.9817743301391602],\n",
    "#     'val_loss': [4.535243034362793, 4.51762580871582, 4.469428539276123, 4.436275482177734, 4.456634521484375, 4.815524578094482, 4.3103556632995605, 4.498797416687012, 4.790141582489014, 4.464589595794678, 4.674554347991943, 4.561259746551514, 4.533383369445801, 4.472135066986084, 4.466953754425049, 4.478504180908203, 4.723540782928467, 4.859069347381592, 4.496937274932861, 4.377903461456299]\n",
    "# }\n",
    "\n",
    "# visualize_training_results(training_results, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93c4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: run_model(test_frames_dfs, \"NN_model_v1\") \n",
    "def run_model_tmp(frames_dfs, model_name):\n",
    "    # Load varibles\n",
    "    numerical_cols, categorical_cols, positions, sequence_length = extract_variables(model_name)\n",
    "\n",
    "    # Load model\n",
    "    model = load_tf_model(f\"models/{model_name}.h5\", euclidean_distance_loss=True)\n",
    "\n",
    "    # Prepared the DataFrames and concatenate into a single large DataFrame\n",
    "    prepared_frames_dfs = [prepare_df(frames_df, numerical_cols, categorical_cols, positions=positions) for frames_df in frames_dfs]\n",
    "    frames_concat_df = pd.concat(prepared_frames_dfs, ignore_index=True)\n",
    "\n",
    "    # Save the original index before sorting\n",
    "    original_index = frames_concat_df.index\n",
    "\n",
    "    # Prepare the input data for LSTM model\n",
    "    if \"LSTM\" in model_name:\n",
    "        X_test_input, y_test = prepare_LSTM_input_data(frames_dfs, numerical_cols, categorical_cols, sequence_length, positions)\n",
    "\n",
    "        # Only keep rows that can be sequentialized\n",
    "        add_can_be_sequentialized(frames_concat_df, sequence_length)\n",
    "\n",
    "        frames_concat_df = frames_concat_df[frames_concat_df[\"can_be_sequentialized\"]]\n",
    "\n",
    "        # Sort the DataFrame by 'team', 'match_id', and most importantly 'player'\n",
    "        frames_concat_df = frames_concat_df.sort_values(by=['team', 'match_id', 'player'])\n",
    "\n",
    "    # Prepare the input data for non-LSTM model\n",
    "    else:\n",
    "        X_test_input, y_test = prepare_EL_input_data(frames_dfs, numerical_cols, categorical_cols, positions)\n",
    "\n",
    "    # Make predictions using the loaded tf model\n",
    "    predictions = model.predict(X_test_input)\n",
    "\n",
    "    # Extract the predicted values\n",
    "    x_future_pred = predictions[:, 0]\n",
    "    y_future_pred = predictions[:, 1]\n",
    "\n",
    "    # Add the predicted values to 'frames_concat_df'\n",
    "    frames_concat_df['x_future_pred'] = x_future_pred\n",
    "    frames_concat_df['y_future_pred'] = y_future_pred\n",
    "\n",
    "    # Clip values to stay on the pitch\n",
    "    frames_concat_df['x_future_pred'] = frames_concat_df['x_future_pred'].clip(lower=0, upper=pitch_length)\n",
    "    frames_concat_df['y_future_pred'] = frames_concat_df['y_future_pred'].clip(lower=0, upper=pitch_width)\n",
    "\n",
    "    # Smooth the predicted coordinates\n",
    "    # smooth_predictions_xy(frames_df, alpha=0.98)\n",
    "\n",
    "    # \"Unsort\" the DataFrame to get it back to its original order\n",
    "    frames_concat_df = frames_concat_df.reindex(original_index)\n",
    "\n",
    "    return y_test, predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
