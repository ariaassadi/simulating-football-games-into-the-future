{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f6979",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the games into train, test, and validtion. This way, each game will be treated seperatly\n",
    "def split_match_ids(match_ids, train_size=0.7, test_size=0.1, val_size=0.2, random_state=42):\n",
    "    # Calculate the remaining size after the test and validation sizes are removed\n",
    "    remaining_size = 1.0 - train_size\n",
    "\n",
    "    # Check if the sum of sizes is not equal to 1\n",
    "    if remaining_size < 0 or abs(train_size + test_size + val_size - 1.0) > 1e-6:\n",
    "        raise ValueError(\"The sum of train_size, test_size, and val_size must be equal to 1.\")\n",
    "    \n",
    "    # Split the match IDs into train, test, and validation sets\n",
    "    train_ids, remaining_ids = train_test_split(match_ids, train_size=train_size, random_state=random_state)\n",
    "    val_ids, test_ids = train_test_split(remaining_ids, test_size=test_size / remaining_size, random_state=random_state)\n",
    "    \n",
    "    return train_ids, test_ids, val_ids\n",
    "    \n",
    "# Get the next model file name based on the number of current models\n",
    "def get_next_model_filename(model_name):\n",
    "    models_folder = \"./models/\"\n",
    "\n",
    "    # Get a list of existing model filenames in the models folder\n",
    "    existing_models = [filename for filename in os.listdir(models_folder) if filename.endswith('.h5') and model_name in filename]\n",
    "\n",
    "    # Determine the number of existing models\n",
    "    num_existing_models = len(existing_models)\n",
    "\n",
    "    # Construct the filename for the next model\n",
    "    next_model_filename = f\"{model_name}_{num_existing_models + 1}.h5\"\n",
    "\n",
    "    return os.path.join(models_folder, next_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef43b7",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed/frames\n",
    "def load_all_processed_frames():\n",
    "    # Create DataFrame for storing all frames\n",
    "    frames_dfs = []\n",
    "    # Load frames_df\n",
    "    for selected_season in seasons:\n",
    "        for selected_competition in competitions:\n",
    "            # Define paths\n",
    "            DATA_FOLDER_PROCESSED = f\"{DATA_LOCAL_FOLDER}/data/{selected_season}/{selected_competition}/processed\"\n",
    "\n",
    "            # Find all frames parquet files\n",
    "            match_paths = glob.glob(os.path.join(DATA_FOLDER_PROCESSED, \"*.parquet\"))\n",
    "\n",
    "            # Extract IDs without the \".parquet\" extension\n",
    "            match_ids = [os.path.splitext(os.path.basename(path))[0] for path in match_paths]\n",
    "            # match_ids = ['49e6bfdf-abf3-499d-b60e-cf727c6523c1']\n",
    "\n",
    "            # For all matches\n",
    "            for match_id in match_ids:\n",
    "                # Convert parquet file to a DataFrame\n",
    "                file_path_match = f\"{DATA_FOLDER_PROCESSED}/{match_id}.parquet\"\n",
    "                frames_df = pd.read_parquet(file_path_match)\n",
    "                \n",
    "                # Append the DataFrame to frames_dfs\n",
    "                frames_dfs.append(frames_df)\n",
    "\n",
    "    return frames_dfs\n",
    "\n",
    "# Load every frames_df to a list\n",
    "frames_dfs = load_all_processed_frames()\n",
    "\n",
    "# Create an internal match_id for each game\n",
    "match_ids = range(len(frames_dfs))\n",
    "\n",
    "# Split match IDs into train, test, and validation sets\n",
    "train_ids, test_ids, val_ids = split_match_ids(match_ids=match_ids)\n",
    "\n",
    "# Select frames data for training, testing, and validation\n",
    "train_frames_dfs = [frames_dfs[i] for i in train_ids]\n",
    "test_frames_dfs = [frames_dfs[i] for i in test_ids]\n",
    "val_frames_dfs = [frames_dfs[i] for i in val_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196e924",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### Dense NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1746338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 22:23:34.191790: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(frames_dfs):\n",
    "    # Define numerical and categorical columns\n",
    "    numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_ran', 'minute']\n",
    "    categorical_cols = ['role', 'team_direction']\n",
    "\n",
    "    # Initialize lists to store features and labels\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for frames_df in frames_dfs:\n",
    "        # Fill NaN values with zeros for numerical columns\n",
    "        frames_df[numerical_cols] = frames_df[numerical_cols].fillna(0)\n",
    "\n",
    "        # Drop rows with NaN values in the labels (y)\n",
    "        frames_df.dropna(subset=['x_future', 'y_future'], inplace=True)\n",
    "\n",
    "        # Extract features and labels from frames_df\n",
    "        X = frames_df[numerical_cols + categorical_cols]\n",
    "        y = frames_df[['x_future', 'y_future']]\n",
    "\n",
    "        # Add features and labels to the lists\n",
    "        X_data.append(X)\n",
    "        y_data.append(y)\n",
    "\n",
    "    # Concatenate the lists to create the final feature and label DataFrame\n",
    "    X_data = pd.concat(X_data)\n",
    "    y_data = pd.concat(y_data)\n",
    "\n",
    "    # Define column transformer for one-hot encoding team_direction\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(), categorical_cols)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create pipeline for preprocessing and apply it to X_data\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    X_data_scaled = pipeline.fit_transform(X_data)\n",
    "\n",
    "    return X_data_scaled, y_data\n",
    "\n",
    "def train_NN(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare the data for training\n",
    "    X_train, y_train = prepare_data(train_frames_dfs)\n",
    "\n",
    "    # Prepare the data for validation\n",
    "    X_val, y_val = prepare_data(val_frames_dfs)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(X_train, y_train, X_val, y_val, val_frames_dfs)\n",
    "\n",
    "def define_model(input_shape):\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, val_frames_dfs):\n",
    "    # Define the model\n",
    "    model = define_model(X_train.shape[1])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model.save(get_next_model_filename(\"NN_model\"))\n",
    "\n",
    "    # Print the error using total_error_loss function\n",
    "    train_error = total_error_loss(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    val_error = total_error_loss(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    print(\"Training Error:\", train_error)\n",
    "    print(\"Validation Error:\", val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5caf3eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN model\n",
    "train_NN(train_frames_dfs, val_frames_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(frames_dfs):\n",
    "    # Define numerical and categorical columns\n",
    "    numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_ran', 'minute']\n",
    "    categorical_cols = ['role', 'team_direction']\n",
    "\n",
    "    # Initialize lists to store features and labels\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for frames_df in frames_dfs:\n",
    "        # Fill NaN values with zeros for numerical columns\n",
    "        frames_df[numerical_cols] = frames_df[numerical_cols].fillna(0)\n",
    "\n",
    "        # Drop rows with NaN values in the labels (y)\n",
    "        frames_df.dropna(subset=['x_future', 'y_future'], inplace=True)\n",
    "\n",
    "        # Extract features and labels from frames_df\n",
    "        X = frames_df[numerical_cols + categorical_cols]\n",
    "        y = frames_df[['x_future', 'y_future']]\n",
    "\n",
    "        # Add features and labels to the lists\n",
    "        X_data.append(X)\n",
    "        y_data.append(y)\n",
    "\n",
    "    # Concatenate the lists to create the final feature and label DataFrame\n",
    "    X_data = pd.concat(X_data)\n",
    "    y_data = pd.concat(y_data)\n",
    "\n",
    "    # Define column transformer for one-hot encoding team_direction\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(), categorical_cols)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create pipeline for preprocessing and apply it to X_data\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    X_data_scaled = pipeline.fit_transform(X_data)\n",
    "\n",
    "    return X_data_scaled, y_data\n",
    "\n",
    "def train_NN(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare the data for training\n",
    "    X_train, y_train = prepare_data(train_frames_dfs)\n",
    "\n",
    "    # Prepare the data for validation\n",
    "    X_val, y_val = prepare_data(val_frames_dfs)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(X_train, y_train, X_val, y_val, val_frames_dfs)\n",
    "\n",
    "def define_model(input_shape):\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, val_frames_dfs):\n",
    "    # Define the model\n",
    "    model = define_model(X_train.shape[1])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model.save(get_next_model_filename(\"NN_model\"))\n",
    "\n",
    "    # Print the error using total_error_loss function\n",
    "    train_error = total_error_loss(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    val_error = total_error_loss(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    print(\"Training Error:\", train_error)\n",
    "    print(\"Validation Error:\", val_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Aug 14 2022, 22:57:54) [GCC 11.3.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
