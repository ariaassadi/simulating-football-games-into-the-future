{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 11:58:27.450249: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, BatchNormalization, Dropout, Reshape, LSTM\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import date\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from utils import load_processed_frames, split_match_ids, get_next_model_filename, euclidean_distance_loss, total_error_loss, define_regularizers, prepare_EL_input_data, prepare_LSTM_input_data, create_embeddings, smooth_predictions_xy, run_model, evaluate_model, print_column_variance\n",
    "from utils import prepare_df, add_can_be_sequentialized, extract_variables, load_tf_model\n",
    "from visualize_game import visualize_training_results\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf81b0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfca7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical, categorical, and y columns\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']\n",
    "categorical_cols = ['team_direction', 'role']\n",
    "\n",
    "# Define parameters for model training\n",
    "n_epochs = 1\n",
    "batch_size = 32\n",
    "n_matches = 10\n",
    "sequence_length = 10    # Sequence length for LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef43b7",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load every frames_df to a list\n",
    "frames_dfs = load_processed_frames(n_matches=n_matches)\n",
    "\n",
    "# Create an internal match_id for each game\n",
    "match_ids = range(len(frames_dfs))\n",
    "\n",
    "# Split match IDs into train, test, and validation sets\n",
    "train_ids, test_ids, val_ids = split_match_ids(match_ids=match_ids)\n",
    "\n",
    "# train_ids, val_ids = [0], [1]\n",
    "\n",
    "# Select frames data for training, testing, and validation\n",
    "train_frames_dfs = [frames_dfs[i] for i in train_ids]\n",
    "test_frames_dfs = [frames_dfs[i] for i in test_ids]\n",
    "val_frames_dfs = [frames_dfs[i] for i in val_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### NN with Embedding layers\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005fb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model with embeddings for categorical features.\n",
    "def define_NN_model_with_embedding(numerical_input_shape, l1=0, l2=0):\n",
    "    if categorical_cols:\n",
    "        categorical_inputs, categorical_flats = create_embeddings(categorical_cols)  # Create embeddings\n",
    "        numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')  # Numerical input\n",
    "        concatenated_features = Concatenate()([*categorical_flats, numerical_input])  # Combine all features\n",
    "        model_inputs = [*categorical_inputs, numerical_input]  # Model inputs\n",
    "    else:\n",
    "        numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')  # Numerical input\n",
    "        concatenated_features = numerical_input  # Use only numerical input\n",
    "        model_inputs = numerical_input  # Model inputs\n",
    "    \n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "\n",
    "    output_layer = Dense(2)(dense_layer_2)  # Output layer for x_future and y_future\n",
    "    model = Model(inputs=model_inputs, outputs=output_layer)  # Build model\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, numerical_cols, categorical_cols, positions=[], l1=0, l2=0, special_text=None):\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_EL_input_data(train_frames_dfs, numerical_cols, categorical_cols, positions=positions)\n",
    "    X_val_input, y_val = prepare_EL_input_data(val_frames_dfs, numerical_cols, categorical_cols, positions=positions)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model_with_embedding(numerical_input_shape=len(numerical_cols), l1=l1, l2=l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        f.write(f\"positions={positions}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078b2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the NN model with embedding layers\n",
    "# n_epochs = 5\n",
    "# categorical_cols = ['position']\n",
    "# positions = [\"Attacking Midfielder\", \"Central Midfielder\", \"Centre-Back\", \"Defensive Midfielder\", \"Forward\", \"Full-Back\", \"Goalkeeper\", \"Wide Midfielder\", \"Winger\"]\n",
    "# # positions = [\"Central Midfielder\", \"Winger\"]\n",
    "\n",
    "# numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'height']\n",
    "# train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, numerical_cols, categorical_cols, positions=positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58786e12",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696c04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = [\"LSTM_model_v2\"]\n",
    "# for model_name in model_names:\n",
    "#     error = evaluate_model(test_frames_dfs, model_name)\n",
    "#     print(f\"{model_name}: {error}\")\n",
    "\n",
    "# Test different alpha values\n",
    "\n",
    "# frames_df = run_model(test_frames_dfs, \"LSTM_model_v5\")\n",
    "# alpha = 1.0\n",
    "# for i in range(10):\n",
    "#     frames_df = frames_df.copy()\n",
    "#     smooth_predictions_xy(frames_df, alpha=alpha)\n",
    "#     error = total_error_loss(frames_df)\n",
    "#     print(f\"{round(alpha, 2)}: {error}\")\n",
    "#     alpha -= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91f8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print column variance for position\n",
    "# column_to_analyze = 'position'\n",
    "# model_name = \"LSTM_model_v9\"\n",
    "# print_column_variance(val_frames_dfs, model_name, column_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ccb12",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### LSTM model\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46e9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NN model with LSTM layer\n",
    "def define_LSTM_model(numerical_input_shape, sequence_length, l1=0, l2=0):  \n",
    "    # Handle case where we have categorical columns\n",
    "    if categorical_cols:\n",
    "        # Create embeddings for categorical data\n",
    "        categorical_inputs, categorical_flats = create_embeddings(categorical_cols)\n",
    "        \n",
    "        # Input for numerical data\n",
    "        numerical_input = Input(shape=(sequence_length, numerical_input_shape), name='numerical_input')\n",
    "\n",
    "        # Processing sequence with LSTM\n",
    "        lstm_out = LSTM(64)(numerical_input)\n",
    "\n",
    "        # Assuming we want to concatenate LSTM output with categorical embeddings\n",
    "        # Note: This might need adjustment based on how you want to use categorical data\n",
    "        concatenated_features = Concatenate()([lstm_out] + categorical_flats)\n",
    "\n",
    "        model_inputs = categorical_inputs + [numerical_input]\n",
    "\n",
    "    # Handle case where we only have numerical columns3\n",
    "    else:\n",
    "        numerical_input = Input(shape=(sequence_length, numerical_input_shape), name='numerical_input')  # Numerical input\n",
    "        lstm_layer = LSTM(64)(numerical_input)  # LSTM layer directly using numerical input\n",
    "        concatenated_features = lstm_layer  # Directly use LSTM output\n",
    "        model_inputs = [numerical_input]  # Model inputs\n",
    "\n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "\n",
    "    output_layer = Dense(2)(dense_layer_2)  # Output layer for x_future and y_future\n",
    "    model = Model(inputs=model_inputs, outputs=output_layer)  # Build model\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(train_frames_dfs, val_frames_dfs, sequence_length, positions=[], l1=0, l2=0, special_text=None):\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_LSTM_input_data(train_frames_dfs, numerical_cols, categorical_cols, sequence_length, positions=positions)\n",
    "    X_val_input, y_val = prepare_LSTM_input_data(val_frames_dfs, numerical_cols, categorical_cols, sequence_length, positions=positions)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(numerical_input_shape=len(numerical_cols), sequence_length=sequence_length, l1=l1, l2=l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"LSTM_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"sequence_length={sequence_length}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        f.write(f\"positions={positions}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5140068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started prepare_data\n",
      "Ended prepare_data, Duration: 16.09 seconds\n",
      "Started sequentialization\n",
      "Ended sequentialization, Duration: 9.49 seconds\n",
      "Started prepare_data\n",
      "Ended prepare_data, Duration: 4.46 seconds\n",
      "Started sequentialization\n",
      "Ended sequentialization, Duration: 1.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 11:59:24.860739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38551 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4b:00.0, compute capability: 8.0\n",
      "2024-04-24 11:59:28.347575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2024-04-24 11:59:28.457219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-04-24 11:59:28.459239: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x14c0140afb40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-24 11:59:28.459258: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2024-04-24 11:59:28.463842: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-24 11:59:28.572568: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43759/43759 - 110s - loss: 2.0309 - val_loss: 1.9640 - 110s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "n_epochs = 1\n",
    "# positions=['Attacking Midfielder', 'Central Midfielder', 'Centre-Back', 'Defensive Midfielder', 'Forward', 'Full-Back', 'Goalkeeper', 'Wide Midfielder', 'Winger']\n",
    "positions=['Forward']\n",
    "categorical_cols = ['position']\n",
    "sequence_length = 10\n",
    "\n",
    "numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball']\n",
    "train_LSTM_model(train_frames_dfs, val_frames_dfs, sequence_length, positions=positions, l2=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e61943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'tiredness_short']\n",
    "# train_LSTM_model(train_frames_dfs, val_frames_dfs, sequence_length, positions=positions, l2=1e-6)\n",
    "\n",
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'v_x_avg', 'v_y_avg']\n",
    "# train_LSTM_model(train_frames_dfs, val_frames_dfs, sequence_length, positions=positions, l2=1e-6)\n",
    "\n",
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'minute', 'sta', 'distance_ran']\n",
    "# train_LSTM_model(train_frames_dfs, val_frames_dfs, sequence_length, positions=positions, l2=1e-6)\n",
    "\n",
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'angle_to_ball']\n",
    "# train_LSTM_model(train_frames_dfs, val_frames_dfs, sequence_length, positions=positions, l2=1e-6)\n",
    "\n",
    "# numerical_cols=['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball', 'tiredness', 'pac', 'acc']\n",
    "# train_LSTM_model(train_frames_dfs, val_frames_dfs, sequence_length, positions=positions, l2=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9539beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"LSTM_model_v7\"\n",
    "# y_test, predictions = run_model_tmp(test_frames_dfs, model_name)\n",
    "# error = tf.reduce_mean(euclidean_distance_loss(y_test, predictions)).numpy()\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad0032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"LSTM_model_v8\"\n",
    "# error = evaluate_model(test_frames_dfs, model_name)\n",
    "# print(f\"{model_name}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d70d0",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1730e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize training results\n",
    "# model_name = 'NN_embedding_model_3'\n",
    "# training_results = {\n",
    "#     'loss': [2.0478146076202393, 2.0088889598846436, 2.0007753372192383, 1.9968146085739136, 1.9937269687652588, 1.9921172857284546, 1.990675687789917, 1.9893001317977905, 1.9881930351257324, 1.9875684976577759, 1.9872304201126099, 1.9865171909332275, 1.9859004020690918, 1.985435128211975, 1.9848004579544067, 1.983401894569397, 1.9824390411376953, 1.9820188283920288, 1.981824517250061, 1.9817743301391602],\n",
    "#     'val_loss': [4.535243034362793, 4.51762580871582, 4.469428539276123, 4.436275482177734, 4.456634521484375, 4.815524578094482, 4.3103556632995605, 4.498797416687012, 4.790141582489014, 4.464589595794678, 4.674554347991943, 4.561259746551514, 4.533383369445801, 4.472135066986084, 4.466953754425049, 4.478504180908203, 4.723540782928467, 4.859069347381592, 4.496937274932861, 4.377903461456299]\n",
    "# }\n",
    "\n",
    "# visualize_training_results(training_results, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: run_model(test_frames_dfs, \"NN_model_v1\") \n",
    "def run_model_tmp(frames_dfs, model_name):\n",
    "    # Load varibles\n",
    "    numerical_cols, categorical_cols, positions, sequence_length = extract_variables(model_name)\n",
    "\n",
    "    # Load model\n",
    "    model = load_tf_model(f\"models/{model_name}.h5\", euclidean_distance_loss=True)\n",
    "\n",
    "    # Prepared the DataFrames and concatenate into a single large DataFrame\n",
    "    prepared_frames_dfs = [prepare_df(frames_df, numerical_cols, categorical_cols, positions=positions) for frames_df in frames_dfs]\n",
    "    frames_concat_df = pd.concat(prepared_frames_dfs, ignore_index=True)\n",
    "\n",
    "    # Save the original index before sorting\n",
    "    original_index = frames_concat_df.index\n",
    "\n",
    "    # Prepare the input data for LSTM model\n",
    "    if \"LSTM\" in model_name:\n",
    "        X_test_input, y_test = prepare_LSTM_input_data(frames_dfs, numerical_cols, categorical_cols, sequence_length, positions)\n",
    "\n",
    "        # Only keep rows that can be sequentialized\n",
    "        add_can_be_sequentialized(frames_concat_df, sequence_length)\n",
    "\n",
    "        frames_concat_df = frames_concat_df[frames_concat_df[\"can_be_sequentialized\"]]\n",
    "\n",
    "        # Sort the DataFrame by 'team', 'match_id', and most importantly 'player'\n",
    "        frames_concat_df = frames_concat_df.sort_values(by=['team', 'match_id', 'player'])\n",
    "\n",
    "    # Prepare the input data for non-LSTM model\n",
    "    else:\n",
    "        X_test_input, y_test = prepare_EL_input_data(frames_dfs, numerical_cols, categorical_cols, positions)\n",
    "\n",
    "    # Make predictions using the loaded tf model\n",
    "    predictions = model.predict(X_test_input)\n",
    "\n",
    "    # Extract the predicted values\n",
    "    x_future_pred = predictions[:, 0]\n",
    "    y_future_pred = predictions[:, 1]\n",
    "\n",
    "    # Add the predicted values to 'frames_concat_df'\n",
    "    frames_concat_df['x_future_pred'] = x_future_pred\n",
    "    frames_concat_df['y_future_pred'] = y_future_pred\n",
    "\n",
    "    # Clip values to stay on the pitch\n",
    "    frames_concat_df['x_future_pred'] = frames_concat_df['x_future_pred'].clip(lower=0, upper=pitch_length)\n",
    "    frames_concat_df['y_future_pred'] = frames_concat_df['y_future_pred'].clip(lower=0, upper=pitch_width)\n",
    "\n",
    "    # Smooth the predicted coordinates\n",
    "    # smooth_predictions_xy(frames_df, alpha=0.98)\n",
    "\n",
    "    # \"Unsort\" the DataFrame to get it back to its original order\n",
    "    frames_concat_df = frames_concat_df.reindex(original_index)\n",
    "\n",
    "    return y_test, predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
