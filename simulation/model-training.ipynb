{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf81b0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical, categorical, and y columns\n",
    "# numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_ran', 'minute', 'frame']\n",
    "# categorical_cols = ['role', 'team', 'team_direction']\n",
    "# y_cols = ['x_future', 'y_future']\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']\n",
    "categorical_cols = ['role', 'team_direction']\n",
    "y_cols = ['x_future', 'y_future']\n",
    "\n",
    "# Define the length of the sequences\n",
    "sequence_length = FPS * seconds_into_the_future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f6979",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the games into train, test, and validtion. This way, each game will be treated seperatly\n",
    "def split_match_ids(match_ids, train_size=0.7, test_size=0.1, val_size=0.2, random_state=42):\n",
    "    # Calculate the remaining size after the test and validation sizes are removed\n",
    "    remaining_size = 1.0 - train_size\n",
    "\n",
    "    # Check if the sum of sizes is not equal to 1\n",
    "    if remaining_size < 0 or abs(train_size + test_size + val_size - 1.0) > 1e-6:\n",
    "        raise ValueError(\"The sum of train_size, test_size, and val_size must be equal to 1.\")\n",
    "    \n",
    "    # Split the match IDs into train, test, and validation sets\n",
    "    train_ids, remaining_ids = train_test_split(match_ids, train_size=train_size, random_state=random_state)\n",
    "    val_ids, test_ids = train_test_split(remaining_ids, test_size=test_size / remaining_size, random_state=random_state)\n",
    "    \n",
    "    return train_ids, test_ids, val_ids\n",
    "    \n",
    "# Get the next model file name based on the number of current models\n",
    "def get_next_model_filename(model_name):\n",
    "    models_folder = \"./models/\"\n",
    "\n",
    "    # Get a list of existing model filenames in the models folder\n",
    "    existing_models = [filename for filename in os.listdir(models_folder) if filename.endswith('.h5') and model_name in filename]\n",
    "\n",
    "    # Determine the number of existing models\n",
    "    num_existing_models = len(existing_models)\n",
    "\n",
    "    # Construct the filename for the next model\n",
    "    next_model_filename = f\"{model_name}_{num_existing_models + 1}.h5\"\n",
    "\n",
    "    return os.path.join(models_folder, next_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data before training\n",
    "def prepare_data(frames_dfs):\n",
    "\n",
    "    # Initialize lists to store features and labels\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # For each game\n",
    "    for frames_df in frames_dfs:\n",
    "        # Fill NaN values with zeros for numerical columns\n",
    "        frames_df[numerical_cols] = frames_df[numerical_cols].fillna(0)\n",
    "\n",
    "        # Drop rows with NaN values in the labels (y)\n",
    "        frames_df.dropna(subset=y_cols, inplace=True)\n",
    "\n",
    "        # Group data by frame and match_id\n",
    "        grouped = frames_df.groupby(['frame'])\n",
    "\n",
    "        for _, group in grouped:\n",
    "            # Extract features and labels from group\n",
    "            X = group[numerical_cols + categorical_cols]\n",
    "            y = group[y_cols]\n",
    "\n",
    "            # Append the data\n",
    "            X_data.append(X)\n",
    "            y_data.append(y)\n",
    "\n",
    "    # Concatenate the lists to create the final feature and label DataFrame\n",
    "    X_data = pd.concat(X_data)\n",
    "    y_data = pd.concat(y_data)\n",
    "\n",
    "    # Define column transformer for one-hot encoding team_direction\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(), categorical_cols)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create pipeline for preprocessing and apply it to X_data\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    X_data_scaled = pipeline.fit_transform(X_data)\n",
    "\n",
    "    return X_data_scaled, y_data\n",
    "\n",
    "# TODO: Test this function. It's straight from the goat\n",
    "def prepare_sequential_data(X_data, y_data, sequence_length):\n",
    "    # Convert pandas DataFrames to NumPy arrays\n",
    "    X_data_np = X_data.to_numpy()\n",
    "    y_data_np = y_data.to_numpy()\n",
    "\n",
    "    data_length = len(X_data)\n",
    "\n",
    "    # Create an array of indices to extract sequences\n",
    "    indices = np.arange(data_length - sequence_length + 1)[:, None] + np.arange(sequence_length)\n",
    "\n",
    "    # Use advanced indexing to extract sequences directly\n",
    "    X_seq = X_data_np[indices]\n",
    "    y_seq = y_data_np[sequence_length - 1:]\n",
    "\n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1288175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for distance wrongly predicted (in metres) for each object\n",
    "def add_pred_error(frames_df):\n",
    "    # Create a vector with the Eculidian distance between the true position and the predicted position\n",
    "    frames_df['pred_error'] = round(((frames_df['x_future_pred'] - frames_df['x_future'])**2 + (frames_df['y_future_pred'] - frames_df['y_future'])**2)**0.5, 2)\n",
    "    \n",
    "# Add a column for distance wrongly predicted (in metres) for each object. Also return average_pred_error\n",
    "def total_error_loss(frames_df, include_ball=False, ball_has_to_be_in_motion=True):\n",
    "    # Add 'pred_error' column if empty\n",
    "    if 'pred_error' not in frames_df:\n",
    "        add_pred_error(frames_df)\n",
    "    \n",
    "    # Create a new column to store modified pred_error values\n",
    "    frames_df['pred_error_tmp'] = frames_df['pred_error']\n",
    "    \n",
    "    # If specified, set pred_error to None for frames where the ball is not in motion\n",
    "    if ball_has_to_be_in_motion:\n",
    "        frames_df.loc[frames_df[\"ball_in_motion\"] != True, 'pred_error_tmp'] = None\n",
    "\n",
    "    # If specified, set pred_error to None for rows where 'team' is 'ball'\n",
    "    if not include_ball:\n",
    "        frames_df.loc[frames_df['team'] == 'ball', 'pred_error_tmp'] = None\n",
    "\n",
    "    # Calculate average pred_error_tmp, excluding rows where pred_error is None\n",
    "    average_pred_error = frames_df['pred_error_tmp'].mean()\n",
    "\n",
    "    # Drop the temporary column\n",
    "    frames_df.drop(columns=['pred_error_tmp'], inplace=True)\n",
    "\n",
    "    return round(average_pred_error, 2)\n",
    "\n",
    "# Use a model to make predictions on a set of games, and print the evaulated error\n",
    "def predict_and_evaluate(model, X_data, frames_dfs, print_message):\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_data)\n",
    "    \n",
    "    # Concatenate the frames DataFrames into a single large DataFrame\n",
    "    frames_concatenated_df = pd.concat(frames_dfs, ignore_index=True)\n",
    "\n",
    "    # Extract the predicted values\n",
    "    x_future_pred = predictions[:, 0]\n",
    "    y_future_pred = predictions[:, 1]\n",
    "\n",
    "    # Add the predicted values to 'frames_concatenated_df'\n",
    "    frames_concatenated_df['x_future_pred'] = x_future_pred\n",
    "    frames_concatenated_df['y_future_pred'] = y_future_pred\n",
    "\n",
    "    # Use the custom error loss function to calculate the error\n",
    "    error = total_error_loss(frames_concatenated_df, include_ball=True, ball_has_to_be_in_motion=True)\n",
    "\n",
    "    # Print error\n",
    "    print(f\"{print_message}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef43b7",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed/frames\n",
    "def load_all_processed_frames():\n",
    "    # Create DataFrame for storing all frames\n",
    "    frames_dfs = []\n",
    "    # Load frames_df\n",
    "    for selected_season in seasons:\n",
    "        for selected_competition in competitions:\n",
    "            # Define paths\n",
    "            DATA_FOLDER_PROCESSED = f\"{DATA_LOCAL_FOLDER}/data/{selected_season}/{selected_competition}/processed\"\n",
    "\n",
    "            # Find all frames parquet files\n",
    "            match_paths = glob.glob(os.path.join(DATA_FOLDER_PROCESSED, \"*.parquet\"))\n",
    "\n",
    "            # Extract IDs without the \".parquet\" extension\n",
    "            match_ids = [os.path.splitext(os.path.basename(path))[0] for path in match_paths][0:10]\n",
    "            # match_ids = ['49e6bfdf-abf3-499d-b60e-cf727c6523c1']\n",
    "\n",
    "            # For all matches\n",
    "            for match_id in match_ids:\n",
    "                # Convert parquet file to a DataFrame\n",
    "                file_path_match = f\"{DATA_FOLDER_PROCESSED}/{match_id}.parquet\"\n",
    "                frames_df = pd.read_parquet(file_path_match)\n",
    "                \n",
    "                # Append the DataFrame to frames_dfs\n",
    "                frames_dfs.append(frames_df)\n",
    "\n",
    "    return frames_dfs\n",
    "\n",
    "# Load every frames_df to a list\n",
    "frames_dfs = load_all_processed_frames()\n",
    "\n",
    "# Create an internal match_id for each game\n",
    "match_ids = range(len(frames_dfs))\n",
    "\n",
    "# Split match IDs into train, test, and validation sets\n",
    "train_ids, test_ids, val_ids = split_match_ids(match_ids=match_ids)\n",
    "\n",
    "# Select frames data for training, testing, and validation\n",
    "train_frames_dfs = [frames_dfs[i] for i in train_ids]\n",
    "test_frames_dfs = [frames_dfs[i] for i in test_ids]\n",
    "val_frames_dfs = [frames_dfs[i] for i in val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "def prepare_all_dfs():\n",
    "    X_train, y_train = prepare_data(train_frames_dfs)\n",
    "    X_val, y_val = prepare_data(val_frames_dfs)\n",
    "    X_test, y_test = prepare_data(test_frames_dfs)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Store the prepared data to parquet files\n",
    "def store_processed_data():\n",
    "    prepared_data_path = f\"{DATA_LOCAL_FOLDER}/prepared_data\"\n",
    "\n",
    "    # Convert NumPy arrays to pandas DataFrames\n",
    "    X_train_df = pd.DataFrame(X_train, columns=numerical_cols + categorical_cols)\n",
    "    y_train_df = pd.DataFrame(y_train, columns=y_cols)\n",
    "    X_val_df   = pd.DataFrame(X_val,   columns=numerical_cols + categorical_cols)\n",
    "    y_val_df   = pd.DataFrame(y_val,   columns=y_cols)\n",
    "    X_test_df  = pd.DataFrame(X_test,  columns=numerical_cols + categorical_cols)\n",
    "    y_test_df  = pd.DataFrame(y_test,  columns=y_cols)\n",
    "\n",
    "    # Store DataFrames in Parquet format\n",
    "    X_train_df.to_parquet(f\"{prepared_data_path}/X_train.parquet\")\n",
    "    y_train_df.to_parquet(f\"{prepared_data_path}/y_train.parquet\")\n",
    "    X_val_df.to_parquet(f\"{prepared_data_path}/X_val.parquet\")\n",
    "    y_val_df.to_parquet(f\"{prepared_data_path}/y_val.parquet\")\n",
    "    X_test_df.to_parquet(f\"{prepared_data_path}/X_test.parquet\")\n",
    "    y_test_df.to_parquet(f\"{prepared_data_path}/y_test.parquet\")\n",
    "\n",
    "# Read the stored parquet files\n",
    "def read_processed_data():\n",
    "    prepared_data_path = f\"{DATA_LOCAL_FOLDER}/prepared_data\"\n",
    "\n",
    "    X_train = pd.read_parquet(f\"{prepared_data_path}/X_train.parquet\")\n",
    "    y_train = pd.read_parquet(f\"{prepared_data_path}/y_train.parquet\")\n",
    "    X_val   = pd.read_parquet(f\"{prepared_data_path}/X_val.parquet\")\n",
    "    y_val   = pd.read_parquet(f\"{prepared_data_path}/y_val.parquet\")\n",
    "    X_test  = pd.read_parquet(f\"{prepared_data_path}/X_test.parquet\")\n",
    "    y_test  = pd.read_parquet(f\"{prepared_data_path}/y_test.parquet\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Prepare, store, and load data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = prepare_all_dfs()\n",
    "# store_processed_data()\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = read_processed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196e924",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### Dense NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1746338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "def define_NN_model(input_shape):\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_NN_model(X_train, y_train, X_val, y_val, train_frames_dfs, val_frames_dfs):\n",
    "    # Define the model\n",
    "    model = define_NN_model(X_train.shape[1])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model and capture the output\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Redirect standard output to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        sys.stdout = f\n",
    "\n",
    "        # Print the output from model.fit\n",
    "        print(history.history)\n",
    "\n",
    "        # Print the error using total_error_loss function\n",
    "        predict_and_evaluate(model, X_train, train_frames_dfs, \"Training Error\")\n",
    "        predict_and_evaluate(model, X_val, val_frames_dfs, \"Validation Error\")\n",
    "\n",
    "        # Reset standard output\n",
    "        sys.stdout = sys.__stdout__\n",
    "\n",
    "# Train the NN model\n",
    "train_NN_model(X_train, y_train, X_val, y_val, train_frames_dfs, val_frames_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LSTM_model(input_shape):\n",
    "    # Define the lengt of the sequence\n",
    "    timesteps = 5 * FPS\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, activation='relu', input_shape=(timesteps, input_shape[1])),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(X_train, y_train, X_val, y_val, val_frames_dfs):\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(X_train.shape[1:])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model.save(get_next_model_filename(\"LSTM_model\"))\n",
    "\n",
    "    # Print the error using total_error_loss function\n",
    "    train_error = total_error_loss(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    val_error = total_error_loss(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    print(\"Training Error:\", train_error)\n",
    "    print(\"Validation Error:\", val_error)\n",
    "\n",
    "def train_LSTM(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare the data for training\n",
    "    X_train, y_train = prepare_data(train_frames_dfs)\n",
    "\n",
    "    # Prepare the data for validation\n",
    "    X_val, y_val = prepare_data(val_frames_dfs)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(X_train, y_train, X_val, y_val, val_frames_dfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
