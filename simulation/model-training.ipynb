{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34725558",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import load_model as keras_load_model\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from visualize_game import visualize_training_results\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf81b0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical, categorical, and y columns\n",
    "# numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball, 'distance_ran', 'minute', 'frame']\n",
    "# categorical_cols = ['role', 'team', 'team_direction']\n",
    "# y_cols = ['x_future', 'y_future']\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y']\n",
    "categorical_cols = ['team_direction', 'role']\n",
    "y_cols = ['x_future', 'y_future']\n",
    "\n",
    "# Define parameters for model training\n",
    "epochs = 20\n",
    "\n",
    "# Define the length of the sequences\n",
    "sequence_length = FPS * seconds_into_the_future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f6979",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the games into train, test, and validtion. This way, each game will be treated seperatly\n",
    "def split_match_ids(match_ids, train_size=0.7, test_size=0.1, val_size=0.2, random_state=42):\n",
    "    # Calculate the remaining size after the test and validation sizes are removed\n",
    "    remaining_size = 1.0 - train_size\n",
    "\n",
    "    # Check if the sum of sizes is not equal to 1\n",
    "    if remaining_size < 0 or abs(train_size + test_size + val_size - 1.0) > 1e-6:\n",
    "        raise ValueError(\"The sum of train_size, test_size, and val_size must be equal to 1.\")\n",
    "    \n",
    "    # Split the match IDs into train, test, and validation sets\n",
    "    train_ids, remaining_ids = train_test_split(match_ids, train_size=train_size, random_state=random_state)\n",
    "    val_ids, test_ids = train_test_split(remaining_ids, test_size=test_size / remaining_size, random_state=random_state)\n",
    "    \n",
    "    return train_ids, test_ids, val_ids\n",
    "    \n",
    "# Get the next model file name based on the number of current models\n",
    "def get_next_model_filename(model_name):\n",
    "    models_folder = \"./models/\"\n",
    "\n",
    "    # Get a list of existing model filenames in the models folder\n",
    "    existing_models = [filename for filename in os.listdir(models_folder) if filename.endswith('.h5') and model_name in filename]\n",
    "\n",
    "    # Determine the number of existing models\n",
    "    num_existing_models = len(existing_models)\n",
    "\n",
    "    # Construct the filename for the next model\n",
    "    next_model_filename = f\"{model_name}_{num_existing_models + 1}.h5\"\n",
    "\n",
    "    return os.path.join(models_folder, next_model_filename)\n",
    "\n",
    "# Loss function for model training\n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data before training\n",
    "def prepare_data(frames_dfs, include_ball=True, ball_has_to_be_in_motion=False):\n",
    "\n",
    "    # Initialize lists to store features and labels\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # For each game\n",
    "    for frames_df in frames_dfs:\n",
    "        # Fill NaN values with zeros for numerical columns\n",
    "        frames_df[numerical_cols] = frames_df[numerical_cols].fillna(0)\n",
    "\n",
    "        # Drop rows with NaN values in the labels (y)\n",
    "        frames_df.dropna(subset=y_cols, inplace=True)\n",
    "\n",
    "        # Drop rows where 'team' is ball, if specified\n",
    "        if not include_ball:\n",
    "            frames_df = frames_df[frames_df['team'] != 'ball']\n",
    "\n",
    "        # Drop rows where ball is not in motion, if specified\n",
    "        if ball_has_to_be_in_motion:\n",
    "            frames_df = frames_df[frames_df['ball_in_motion']]\n",
    "\n",
    "        # Extract features and labels from group\n",
    "        X = frames_df[numerical_cols + categorical_cols]\n",
    "        y = frames_df[y_cols]\n",
    "\n",
    "        # Append the data\n",
    "        X_data.append(X)\n",
    "        y_data.append(y)\n",
    "\n",
    "    # Concatenate the lists to create the final feature and label DataFrame\n",
    "    X_data_df = pd.concat(X_data)\n",
    "    y_data_df = pd.concat(y_data)\n",
    "\n",
    "    # Apply label encoding to categorical variables\n",
    "    for col in categorical_cols:\n",
    "        label_encoder = LabelEncoder()\n",
    "        X_data_df[col] = label_encoder.fit_transform(X_data_df[col])\n",
    "\n",
    "    # Define column transformer for standard scaling numerical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create pipeline for preprocessing and apply it to X_data\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    X_data_scaled = pipeline.fit_transform(X_data_df)\n",
    "\n",
    "    # Retrieve the transformed feature names from ColumnTransformer\n",
    "    transformed_column_names = numerical_cols + categorical_cols\n",
    "\n",
    "    # Create a DataFrame from the preprocessed feature data\n",
    "    X_data_scaled_df = pd.DataFrame(X_data_scaled, columns=transformed_column_names)\n",
    "\n",
    "    # Convert categorical columns to int\n",
    "    X_data_scaled_df[categorical_cols] = X_data_scaled_df[categorical_cols].astype('int8')\n",
    "\n",
    "    return X_data_scaled_df, y_data_df\n",
    "\n",
    "# TODO: Test this function. It's straight from the goat\n",
    "def prepare_sequential_data(X_data, y_data, sequence_length):\n",
    "    # Convert pandas DataFrames to NumPy arrays\n",
    "    X_data_np = X_data.to_numpy()\n",
    "    y_data_np = y_data.to_numpy()\n",
    "\n",
    "    data_length = len(X_data)\n",
    "\n",
    "    # Create an array of indices to extract sequences\n",
    "    indices = np.arange(data_length - sequence_length + 1)[:, None] + np.arange(sequence_length)\n",
    "\n",
    "    # Use advanced indexing to extract sequences directly\n",
    "    X_seq = X_data_np[indices]\n",
    "    y_seq = y_data_np[sequence_length - 1:]\n",
    "\n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1288175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for distance wrongly predicted (in metres) for each object\n",
    "def add_pred_error(frames_df):\n",
    "    # Create a vector with the Eculidian distance between the true position and the predicted position\n",
    "    frames_df['pred_error'] = round(((frames_df['x_future_pred'] - frames_df['x_future'])**2 + (frames_df['y_future_pred'] - frames_df['y_future'])**2)**0.5, 2)\n",
    "    \n",
    "# Add a column for distance wrongly predicted (in metres) for each object. Also return average_pred_error\n",
    "def total_error_loss(frames_df, include_ball=False, ball_has_to_be_in_motion=True):\n",
    "    # Add 'pred_error' column if empty\n",
    "    if 'pred_error' not in frames_df:\n",
    "        add_pred_error(frames_df)\n",
    "    \n",
    "    # Create a new column to store modified pred_error values\n",
    "    frames_df['pred_error_tmp'] = frames_df['pred_error']\n",
    "    \n",
    "    # If specified, set pred_error to None for frames where the ball is not in motion\n",
    "    if ball_has_to_be_in_motion:\n",
    "        frames_df.loc[frames_df[\"ball_in_motion\"] != True, 'pred_error_tmp'] = None\n",
    "\n",
    "    # If specified, set pred_error to None for rows where 'team' is 'ball'\n",
    "    if not include_ball:\n",
    "        frames_df.loc[frames_df['team'] == 'ball', 'pred_error_tmp'] = None\n",
    "\n",
    "    # Calculate average pred_error_tmp, excluding rows where pred_error is None\n",
    "    average_pred_error = frames_df['pred_error_tmp'].mean()\n",
    "\n",
    "    # Drop the temporary column\n",
    "    frames_df.drop(columns=['pred_error_tmp'], inplace=True)\n",
    "\n",
    "    return round(average_pred_error, 2)\n",
    "\n",
    "# Use a model to make predictions on a set of games, and calculate the error\n",
    "def predict_and_evaluate(model, X_data, frames_dfs, include_ball=False, ball_has_to_be_in_motion=True):\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_data)\n",
    "    \n",
    "    # Concatenate the frames DataFrames into a single large DataFrame\n",
    "    frames_concatenated_df = pd.concat(frames_dfs, ignore_index=True)\n",
    "\n",
    "    # Extract the predicted values\n",
    "    x_future_pred = predictions[:, 0]\n",
    "    y_future_pred = predictions[:, 1]\n",
    "\n",
    "    # Add the predicted values to 'frames_concatenated_df'\n",
    "    frames_concatenated_df['x_future_pred'] = x_future_pred\n",
    "    frames_concatenated_df['y_future_pred'] = y_future_pred\n",
    "\n",
    "    # Calculate error\n",
    "    error = total_error_loss(frames_concatenated_df, include_ball, ball_has_to_be_in_motion)\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef43b7",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed/frames\n",
    "def load_all_processed_frames():\n",
    "    # Create DataFrame for storing all frames\n",
    "    frames_dfs = []\n",
    "    # Load frames_df\n",
    "    for selected_season in seasons:\n",
    "        for selected_competition in competitions:\n",
    "            # Define paths\n",
    "            DATA_FOLDER_PROCESSED = f\"{DATA_LOCAL_FOLDER}/data/{selected_season}/{selected_competition}/processed\"\n",
    "\n",
    "            # Find all frames parquet files\n",
    "            match_paths = glob.glob(os.path.join(DATA_FOLDER_PROCESSED, \"*.parquet\"))\n",
    "\n",
    "            # Extract IDs without the \".parquet\" extension\n",
    "            match_ids = [os.path.splitext(os.path.basename(path))[0] for path in match_paths][0:10]\n",
    "            # match_ids = ['49e6bfdf-abf3-499d-b60e-cf727c6523c1']\n",
    "\n",
    "            # For all matches\n",
    "            for match_id in match_ids:\n",
    "                # Convert parquet file to a DataFrame\n",
    "                file_path_match = f\"{DATA_FOLDER_PROCESSED}/{match_id}.parquet\"\n",
    "                frames_df = pd.read_parquet(file_path_match)\n",
    "                \n",
    "                # Append the DataFrame to frames_dfs\n",
    "                frames_dfs.append(frames_df)\n",
    "\n",
    "    return frames_dfs\n",
    "\n",
    "# Load every frames_df to a list\n",
    "frames_dfs = load_all_processed_frames()\n",
    "\n",
    "# Create an internal match_id for each game\n",
    "match_ids = range(len(frames_dfs))\n",
    "\n",
    "# Split match IDs into train, test, and validation sets\n",
    "train_ids, test_ids, val_ids = split_match_ids(match_ids=match_ids)\n",
    "\n",
    "# Select frames data for training, testing, and validation\n",
    "train_frames_dfs = [frames_dfs[i] for i in train_ids]\n",
    "test_frames_dfs = [frames_dfs[i] for i in test_ids]\n",
    "val_frames_dfs = [frames_dfs[i] for i in val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "def prepare_all_dfs():\n",
    "    X_train, y_train = prepare_data(train_frames_dfs)\n",
    "    X_val, y_val = prepare_data(val_frames_dfs)\n",
    "    X_test, y_test = prepare_data(test_frames_dfs)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Store the prepared data to parquet files\n",
    "def store_prepared_data(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    prepared_data_path = f\"{DATA_LOCAL_FOLDER}/data/prepared_data\"\n",
    "\n",
    "    # Store DataFrames in Parquet format\n",
    "    X_train.to_parquet(f\"{prepared_data_path}/X_train.parquet\")\n",
    "    y_train.to_parquet(f\"{prepared_data_path}/y_train.parquet\")\n",
    "    X_val.to_parquet(f\"{prepared_data_path}/X_val.parquet\")\n",
    "    y_val.to_parquet(f\"{prepared_data_path}/y_val.parquet\")\n",
    "    X_test.to_parquet(f\"{prepared_data_path}/X_test.parquet\")\n",
    "    y_test.to_parquet(f\"{prepared_data_path}/y_test.parquet\")\n",
    "\n",
    "# Read the stored parquet files\n",
    "def read_prepared_data():\n",
    "    prepared_data_path = f\"{DATA_LOCAL_FOLDER}/data/prepared_data\"\n",
    "\n",
    "    X_train = pd.read_parquet(f\"{prepared_data_path}/X_train.parquet\")\n",
    "    y_train = pd.read_parquet(f\"{prepared_data_path}/y_train.parquet\")\n",
    "    X_val   = pd.read_parquet(f\"{prepared_data_path}/X_val.parquet\")\n",
    "    y_val   = pd.read_parquet(f\"{prepared_data_path}/y_val.parquet\")\n",
    "    X_test  = pd.read_parquet(f\"{prepared_data_path}/X_test.parquet\")\n",
    "    y_test  = pd.read_parquet(f\"{prepared_data_path}/y_test.parquet\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Prepare, store, and load data\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = prepare_all_dfs()\n",
    "# store_prepared_data(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = read_prepared_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196e924",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### Dense NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1746338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_NN_model(input_shape):\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_NN_model(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare the data\n",
    "    X_train, y_train = prepare_data(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    X_val, y_val = prepare_data(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "\n",
    "    # Only keep numerical columns\n",
    "    X_train = X_train[numerical_cols]\n",
    "    X_val = X_val[numerical_cols]\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model(X_train.shape[1])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model and capture the output\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=32, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the current global variables to begging of the output file\n",
    "        f.write(f\"epochs={epochs}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "\n",
    "        # Write the output from model.fit\n",
    "        f.write(str(history.history) + '\\n')\n",
    "\n",
    "        # Make predictions and evaluate the validation error\n",
    "        train_error = predict_and_evaluate(model, X_train, train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "        f.write(f\"Training error: {train_error}\\n\")\n",
    "\n",
    "        # Make predictions and evaluate the validation error\n",
    "        val_error = predict_and_evaluate(model, X_val, val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "        f.write(f\"Validation error: {val_error}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaadca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN model\n",
    "train_NN_model(train_frames_dfs, val_frames_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### Embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005fb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_for_embeddings(X_data_df):\n",
    "    # Split the DataFrame into numerical and categorical components\n",
    "    X_numerical = X_data_df.drop(columns=categorical_cols)\n",
    "    X_categorical = {col: X_data_df[col].values for col in categorical_cols}\n",
    "    \n",
    "    return X_numerical, X_categorical\n",
    "\n",
    "def define_NN_model_with_embedding(numerical_input_shape, n_team_directions=3, n_roles=13):\n",
    "    # Inputs\n",
    "    team_direction_input = Input(shape=(1,), name='team_direction_input')\n",
    "    role_input = Input(shape=(1,), name='role_input')\n",
    "    numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')\n",
    "    \n",
    "    # Embeddings\n",
    "    team_direction_embedding = Embedding(input_dim=n_team_directions, output_dim=2, name='team_direction_embedding')(team_direction_input)\n",
    "    role_embedding = Embedding(input_dim=n_roles, output_dim=5, name='role_embedding')(role_input)\n",
    "    \n",
    "    # Flatten the embedding outputs\n",
    "    team_direction_flat = Flatten()(team_direction_embedding)\n",
    "    role_flat = Flatten()(role_embedding)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    concatenated_features = Concatenate()([team_direction_flat, role_flat, numerical_input])\n",
    "    \n",
    "    # Dense layers\n",
    "    dense_layer_1 = Dense(64, activation='relu')(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu')(dense_layer_1)\n",
    "    output_layer = Dense(2)(dense_layer_2)  # Assuming 2 units for x_future and y_future\n",
    "    \n",
    "    # Model\n",
    "    model = Model(inputs=[team_direction_input, role_input, numerical_input], outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare data\n",
    "    X_train, y_train = prepare_data(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    X_val, y_val = prepare_data(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "\n",
    "    # Adjust for embeddings for both training and validation sets\n",
    "    X_train_numerical, X_train_categorical = adjust_for_embeddings(X_train)\n",
    "    X_val_numerical, X_val_categorical = adjust_for_embeddings(X_val)\n",
    "\n",
    "    # Construct input data suitable for the embedding layers\n",
    "    X_train_input = [X_train_categorical['team_direction'].reshape(-1, 1), X_train_categorical['role'].reshape(-1, 1), X_train_numerical]\n",
    "    X_val_input = [X_val_categorical['team_direction'].reshape(-1, 1), X_val_categorical['role'].reshape(-1, 1), X_val_numerical]\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model_with_embedding(numerical_input_shape=X_train_numerical.shape[1])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=epochs, batch_size=32, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_embedding_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "            # Write the current global variables to begging of the output file\n",
    "        f.write(f\"epochs={epochs}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "\n",
    "        # Write the output from model.fit\n",
    "        f.write(str(history.history) + '\\n')\n",
    "\n",
    "        # Make predictions and evaluate the training error\n",
    "        train_error = predict_and_evaluate(model, X_train_input, train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "        f.write(f\"Training error: {train_error}\\n\")\n",
    "\n",
    "        # Make predictions and evaluate the validation error\n",
    "        val_error = predict_and_evaluate(model, X_val_input, val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "        f.write(f\"Validation error: {val_error}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN model with embedding layers\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ccb12",
   "metadata": {},
   "source": [
    "## Predictive model 3\n",
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LSTM_model(input_shape):\n",
    "    # Define the lengt of the sequence\n",
    "    timesteps = 5 * FPS\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, activation='relu', input_shape=(timesteps, input_shape[1])),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(X_train, y_train, X_val, y_val, val_frames_dfs):\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(X_train.shape[1:])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=32, verbose=0)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model.save(get_next_model_filename(\"LSTM_model\"))\n",
    "\n",
    "    # Print the error using total_error_loss function\n",
    "    train_error = total_error_loss(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    val_error = total_error_loss(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    print(\"Training Error:\", train_error)\n",
    "    print(\"Validation Error:\", val_error)\n",
    "\n",
    "def train_LSTM(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare the data for training\n",
    "    X_train, y_train = prepare_data(train_frames_dfs)\n",
    "\n",
    "    # Prepare the data for validation\n",
    "    X_val, y_val = prepare_data(val_frames_dfs)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(X_train, y_train, X_val, y_val, val_frames_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d70d0",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize training results\n",
    "# model_name = 'NN_model_3'\n",
    "# training_results = {\n",
    "#     'loss': [11.86983871459961, 11.278080940246582, 11.235326766967773, 11.211169242858887, 11.196228981018066],\n",
    "#     'val_loss': [11.717867851257324, 11.291694641113281, 11.279356956481934, 11.498793601989746, 11.746583938598633]\n",
    "# }\n",
    "\n",
    "# visualize_training_results(training_results, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Feb  1 2024, 03:10:29) [GCC 11.3.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
