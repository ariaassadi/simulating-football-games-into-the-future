{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34725558",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 16:22:28.193210: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import load_model as keras_load_model\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from datetime import date\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from visualize_game import visualize_training_results\n",
    "from utils import load_processed_frames, split_match_ids, get_next_model_filename, euclidean_distance_loss, adjust_for_embeddings\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf81b0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfca7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical, categorical, and y columns\n",
    "# numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y', 'distance_to_ball, 'angle_to_ball, 'distance_ran', 'minute', 'frame']\n",
    "numerical_cols = ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']\n",
    "categorical_cols = ['team_direction', 'role']\n",
    "y_cols = ['x_future', 'y_future']\n",
    "\n",
    "# Define parameters for model training\n",
    "n_epochs = 2\n",
    "batch_size = 32\n",
    "n_matches = 120\n",
    "\n",
    "# Define the length of the sequences\n",
    "sequence_length = FPS * seconds_into_the_future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f6979",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2cf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data before training\n",
    "def prepare_data(frames_dfs, include_ball=True, ball_has_to_be_in_motion=False):\n",
    "\n",
    "    # Initialize lists to store features and labels\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # For each game\n",
    "    for frames_df in frames_dfs:\n",
    "        # Fill NaN values with zeros for numerical columns\n",
    "        frames_df[numerical_cols] = frames_df[numerical_cols].fillna(0)\n",
    "\n",
    "        # Drop rows with NaN values in the labels (y)\n",
    "        frames_df.dropna(subset=y_cols, inplace=True)\n",
    "\n",
    "        # Drop rows where 'team' is ball, if specified\n",
    "        if not include_ball:\n",
    "            frames_df = frames_df[frames_df['team'] != 'ball']\n",
    "\n",
    "        # Drop rows where ball is not in motion, if specified\n",
    "        if ball_has_to_be_in_motion:\n",
    "            frames_df = frames_df[frames_df['ball_in_motion']]\n",
    "\n",
    "        # Extract features and labels from group\n",
    "        X = frames_df[numerical_cols + categorical_cols]\n",
    "        y = frames_df[y_cols]\n",
    "\n",
    "        # Append the data\n",
    "        X_data.append(X)\n",
    "        y_data.append(y)\n",
    "\n",
    "    # Concatenate the lists to create the final feature and label DataFrame\n",
    "    X_data_df = pd.concat(X_data)\n",
    "    y_data_df = pd.concat(y_data)\n",
    "\n",
    "    # Apply label encoding to categorical variables\n",
    "    for col in categorical_cols:\n",
    "        label_encoder = LabelEncoder()\n",
    "        X_data_df[col] = label_encoder.fit_transform(X_data_df[col])\n",
    "\n",
    "    # Define column transformer for standard scaling numerical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create pipeline for preprocessing and apply it to X_data\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    X_data_scaled = pipeline.fit_transform(X_data_df)\n",
    "\n",
    "    # Retrieve the transformed feature names from ColumnTransformer\n",
    "    transformed_column_names = numerical_cols + categorical_cols\n",
    "\n",
    "    # Create a DataFrame from the preprocessed feature data\n",
    "    X_data_scaled_df = pd.DataFrame(X_data_scaled, columns=transformed_column_names)\n",
    "\n",
    "    # Convert categorical columns to int\n",
    "    X_data_scaled_df[categorical_cols] = X_data_scaled_df[categorical_cols].astype('int8')\n",
    "\n",
    "    return X_data_scaled_df, y_data_df\n",
    "\n",
    "# TODO: Test this function. It's straight from the goat\n",
    "def prepare_sequential_data(X_data, y_data, sequence_length):\n",
    "    # Convert pandas DataFrames to NumPy arrays\n",
    "    X_data_np = X_data.to_numpy()\n",
    "    y_data_np = y_data.to_numpy()\n",
    "\n",
    "    data_length = len(X_data)\n",
    "\n",
    "    # Create an array of indices to extract sequences\n",
    "    indices = np.arange(data_length - sequence_length + 1)[:, None] + np.arange(sequence_length)\n",
    "\n",
    "    # Use advanced indexing to extract sequences directly\n",
    "    X_seq = X_data_np[indices]\n",
    "    y_seq = y_data_np[sequence_length - 1:]\n",
    "\n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1288175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for distance wrongly predicted (in metres) for each object\n",
    "def add_pred_error(frames_df):\n",
    "    # Create a vector with the Eculidian distance between the true position and the predicted position\n",
    "    frames_df['pred_error'] = round(((frames_df['x_future_pred'] - frames_df['x_future'])**2 + (frames_df['y_future_pred'] - frames_df['y_future'])**2)**0.5, 2)\n",
    "    \n",
    "# Add a column for distance wrongly predicted (in metres) for each object. Also return average_pred_error\n",
    "def total_error_loss(frames_df, include_ball=False, ball_has_to_be_in_motion=True):\n",
    "    # Add 'pred_error' column\n",
    "    add_pred_error(frames_df)\n",
    "    \n",
    "    # Create a new column to store modified pred_error values\n",
    "    frames_df['pred_error_tmp'] = frames_df['pred_error']\n",
    "    \n",
    "    # If specified, set pred_error to None for frames where the ball is not in motion\n",
    "    if ball_has_to_be_in_motion:\n",
    "        frames_df.loc[frames_df[\"ball_in_motion\"] != True, 'pred_error_tmp'] = None\n",
    "\n",
    "    # If specified, set pred_error to None for rows where 'team' is 'ball'\n",
    "    if not include_ball:\n",
    "        frames_df.loc[frames_df['team'] == 'ball', 'pred_error_tmp'] = None\n",
    "\n",
    "    # Calculate average pred_error_tmp, excluding rows where pred_error is None\n",
    "    average_pred_error = frames_df['pred_error_tmp'].mean()\n",
    "\n",
    "    # Drop the temporary column\n",
    "    frames_df.drop(columns=['pred_error_tmp'], inplace=True)\n",
    "\n",
    "    return round(average_pred_error, 2)\n",
    "\n",
    "# Use a model to make predictions on a set of games, and calculate the error\n",
    "def predict_and_evaluate(model, X_data, frames_dfs, include_ball=False, ball_has_to_be_in_motion=True):\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_data)\n",
    "    \n",
    "    # For each game\n",
    "    for idx, frames_df in enumerate(frames_dfs):\n",
    "        # Fill NaN values with zeros for numerical columns\n",
    "        frames_df[numerical_cols] = frames_df[numerical_cols].fillna(0)\n",
    "\n",
    "        # Drop rows with NaN values in the labels (y)\n",
    "        frames_df.dropna(subset=y_cols, inplace=True)\n",
    "\n",
    "        # Drop rows where 'team' is ball, if specified\n",
    "        if not include_ball:\n",
    "            frames_dfs[idx] = frames_df.loc[frames_df['team'] != 'ball']\n",
    "\n",
    "        # Drop rows where ball is not in motion, if specified\n",
    "        if ball_has_to_be_in_motion:\n",
    "            frames_dfs[idx] = frames_dfs[idx].loc[frames_dfs[idx]['ball_in_motion']]\n",
    "\n",
    "    # Concatenate the frames DataFrames into a single large DataFrame\n",
    "    frames_concatenated_df = pd.concat(frames_dfs, ignore_index=True)\n",
    "\n",
    "    # Extract the predicted values\n",
    "    x_future_pred = predictions[:, 0]\n",
    "    y_future_pred = predictions[:, 1]\n",
    "\n",
    "    # Add the predicted values to 'frames_concatenated_df'\n",
    "    frames_concatenated_df['x_future_pred'] = x_future_pred\n",
    "    frames_concatenated_df['y_future_pred'] = y_future_pred\n",
    "\n",
    "    # Calculate error\n",
    "    error = total_error_loss(frames_concatenated_df, include_ball, ball_has_to_be_in_motion)\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef43b7",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load every frames_df to a list\n",
    "frames_dfs = load_processed_frames(n_matches=n_matches)\n",
    "\n",
    "# Create an internal match_id for each game\n",
    "match_ids = range(len(frames_dfs))\n",
    "\n",
    "# Split match IDs into train, test, and validation sets\n",
    "train_ids, test_ids, val_ids = split_match_ids(match_ids=match_ids)\n",
    "\n",
    "# Select frames data for training, testing, and validation\n",
    "train_frames_dfs = [frames_dfs[i] for i in train_ids]\n",
    "# test_frames_dfs = [frames_dfs[i] for i in test_ids]\n",
    "val_frames_dfs = [frames_dfs[i] for i in val_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196e924",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### Dense NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1746338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_NN_model(input_shape):\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_NN_model(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare the data\n",
    "    X_train, y_train = prepare_data(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    X_val, y_val = prepare_data(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "\n",
    "    # Only keep numerical columns\n",
    "    X_train = X_train[numerical_cols]\n",
    "    X_val = X_val[numerical_cols]\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model(X_train.shape[1])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model and capture the output\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"Training results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccaadca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN model\n",
    "# train_NN_model(train_frames_dfs, val_frames_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### Embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "005fb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_NN_model_with_embedding(numerical_input_shape, l1=0, l2=0, n_team_directions=2, n_roles=13):\n",
    "    # Inputs\n",
    "    team_direction_input = Input(shape=(1,), name='team_direction_input')\n",
    "    role_input = Input(shape=(1,), name='role_input')\n",
    "    numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')\n",
    "    \n",
    "    # Embeddings\n",
    "    team_direction_embedding = Embedding(input_dim=n_team_directions, output_dim=2, name='team_direction_embedding')(team_direction_input)\n",
    "    role_embedding = Embedding(input_dim=n_roles, output_dim=5, name='role_embedding')(role_input)\n",
    "    \n",
    "    # Flatten the embedding outputs\n",
    "    team_direction_flat = Flatten()(team_direction_embedding)\n",
    "    role_flat = Flatten()(role_embedding)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    concatenated_features = Concatenate()([team_direction_flat, role_flat, numerical_input])\n",
    "    \n",
    "    # Define regularizer (prioritize l1)\n",
    "    if l1 != 0:\n",
    "        regularizer = regularizers.l1(l1)\n",
    "    elif l2 != 0:\n",
    "        regularizer = regularizers.l2(l2)\n",
    "    else:\n",
    "        regularizer = None\n",
    "\n",
    "    # Dense layers\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "    output_layer = Dense(2)(dense_layer_2)  # Assuming 2 units for x_future and y_future\n",
    "    \n",
    "    # Model\n",
    "    model = Model(inputs=[team_direction_input, role_input, numerical_input], outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, l1=0, l2=0):\n",
    "    # Prepare data\n",
    "    X_train, y_train = prepare_data(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    X_val, y_val = prepare_data(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "\n",
    "    # Adjust for embeddings for both training and validation sets\n",
    "    X_train_numerical, X_train_categorical = adjust_for_embeddings(X_train, categorical_cols)\n",
    "    X_val_numerical, X_val_categorical = adjust_for_embeddings(X_val, categorical_cols)\n",
    "\n",
    "    # Construct input data suitable for the embedding layers\n",
    "    X_train_input = [X_train_categorical['team_direction'].reshape(-1, 1), X_train_categorical['role'].reshape(-1, 1), X_train_numerical]\n",
    "    X_val_input = [X_val_categorical['team_direction'].reshape(-1, 1), X_val_categorical['role'].reshape(-1, 1), X_val_numerical]\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model_with_embedding(numerical_input_shape=X_train_numerical.shape[1], l1=l1, l2=l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance_loss)\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"NN_embedding_model\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"matches={n_matches}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "078b2f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 16:28:40.908951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79261 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 16:29:00.684800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-15 16:29:00.686862: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x14254fb955e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-15 16:29:00.686882: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-03-15 16:29:00.691576: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-15 16:29:00.826020: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the NN model with embedding layers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs)\n\u001b[1;32m      3\u001b[0m train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, l1\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, l1\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)\n",
      "Cell \u001b[0;32mIn [10], line 63\u001b[0m, in \u001b[0;36mtrain_NN_model_with_embedding\u001b[0;34m(train_frames_dfs, val_frames_dfs, l1, l2)\u001b[0m\n\u001b[1;32m     60\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39meuclidean_distance_loss)\n\u001b[1;32m     62\u001b[0m \u001b[39m# Train the model with the corrected input format\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train_input, y_train, validation_data\u001b[39m=\u001b[39;49m(X_val_input, y_val), epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     65\u001b[0m \u001b[39m# Save the trained model to disk\u001b[39;00m\n\u001b[1;32m     66\u001b[0m model_filename \u001b[39m=\u001b[39m get_next_model_filename(\u001b[39m\"\u001b[39m\u001b[39mNN_embedding_model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/apps/Arch/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/apps/Arch/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/keras/engine/training.py:1641\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1639\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1640\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1641\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   1642\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m         ):\n\u001b[1;32m   1649\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m/apps/Arch/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/keras/engine/data_adapter.py:1371\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1371\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m   1372\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[1;32m   1373\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1374\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[1;32m   1376\u001b[0m )\n\u001b[1;32m   1378\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m/apps/Arch/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:639\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    640\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    641\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/apps/Arch/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1155\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[1;32m   1134\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/apps/Arch/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1121\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1120\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1122\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the NN model with embedding layers\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs)\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, l1=0.001)\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, l1=0.0001)\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, l2=0.0001)\n",
    "train_NN_model_with_embedding(train_frames_dfs, val_frames_dfs, l2=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ccb12",
   "metadata": {},
   "source": [
    "## Predictive model 3\n",
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LSTM_model(input_shape):\n",
    "    # Define the lengt of the sequence\n",
    "    timesteps = 5 * FPS\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, activation='relu', input_shape=(timesteps, input_shape[1])),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2)  # Output layer with 2 units for x_future and y_future\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(X_train, y_train, X_val, y_val, val_frames_dfs):\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(X_train.shape[1:])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model.save(get_next_model_filename(\"LSTM_model\"))\n",
    "\n",
    "    # Print the error using total_error_loss function\n",
    "    train_error = total_error_loss(train_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    val_error = total_error_loss(val_frames_dfs, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "    print(\"Training Error:\", train_error)\n",
    "    print(\"Validation Error:\", val_error)\n",
    "\n",
    "def train_LSTM(train_frames_dfs, val_frames_dfs):\n",
    "    # Prepare the data for training\n",
    "    X_train, y_train = prepare_data(train_frames_dfs)\n",
    "\n",
    "    # Prepare the data for validation\n",
    "    X_val, y_val = prepare_data(val_frames_dfs)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(X_train, y_train, X_val, y_val, val_frames_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d70d0",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize training results\n",
    "# model_name = 'NN_embedding_model_3'\n",
    "# training_results = {\n",
    "#     'loss': [2.0478146076202393, 2.0088889598846436, 2.0007753372192383, 1.9968146085739136, 1.9937269687652588, 1.9921172857284546, 1.990675687789917, 1.9893001317977905, 1.9881930351257324, 1.9875684976577759, 1.9872304201126099, 1.9865171909332275, 1.9859004020690918, 1.985435128211975, 1.9848004579544067, 1.983401894569397, 1.9824390411376953, 1.9820188283920288, 1.981824517250061, 1.9817743301391602],\n",
    "#     'val_loss': [4.535243034362793, 4.51762580871582, 4.469428539276123, 4.436275482177734, 4.456634521484375, 4.815524578094482, 4.3103556632995605, 4.498797416687012, 4.790141582489014, 4.464589595794678, 4.674554347991943, 4.561259746551514, 4.533383369445801, 4.472135066986084, 4.466953754425049, 4.478504180908203, 4.723540782928467, 4.859069347381592, 4.496937274932861, 4.377903461456299]\n",
    "# }\n",
    "\n",
    "# visualize_training_results(training_results, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Feb  1 2024, 03:10:29) [GCC 11.3.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
