{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f6979",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca3207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the games into train, test, and validtion. This way, each game will be treated seperatly\n",
    "def split_match_ids(match_ids, train_size=0.7, test_size=0.1, val_size=0.2, random_state=42):\n",
    "    # Calculate the remaining size after the test and validation sizes are removed\n",
    "    remaining_size = 1.0 - train_size\n",
    "\n",
    "    # Check if the sum of sizes is not equal to 1\n",
    "    if remaining_size < 0 or abs(train_size + test_size + val_size - 1.0) > 1e-6:\n",
    "        raise ValueError(\"The sum of train_size, test_size, and val_size must be equal to 1.\")\n",
    "    \n",
    "    # Split the match IDs into train, test, and validation sets\n",
    "    train_ids, remaining_ids = train_test_split(match_ids, train_size=train_size, random_state=random_state)\n",
    "    val_ids, test_ids = train_test_split(remaining_ids, test_size=test_size / remaining_size, random_state=random_state)\n",
    "    \n",
    "    return train_ids, test_ids, val_ids\n",
    "    \n",
    "# Get the latest model, for a given 'model_name', based on the number of current models\n",
    "def get_latest_model_filename(model_name):\n",
    "    models_folder = \"./models/\"\n",
    "\n",
    "    # Get a list of existing model filenames in the models folder\n",
    "    existing_models = [filename for filename in os.listdir(models_folder) if filename.startswith(model_name) and filename.endswith('.h5')]\n",
    "\n",
    "    # Sort the existing models by name\n",
    "    existing_models.sort()\n",
    "\n",
    "    if existing_models:\n",
    "        # Get the latest model filename\n",
    "        latest_model_filename = existing_models[-1]\n",
    "    else:\n",
    "        ValueError(f\"No existing model available with the given name: models/{model_name}_XX.h5\")\n",
    "\n",
    "    return os.path.join(models_folder, latest_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef43b7",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed/frames\n",
    "def load_all_processed_frames():\n",
    "    # Create DataFrame for storing all frames\n",
    "    frames_dfs = []\n",
    "    # Load frames_df\n",
    "    for selected_season in seasons:\n",
    "        for selected_competition in competitions:\n",
    "            # Define paths\n",
    "            DATA_FOLDER_PROCESSED = f\"{DATA_LOCAL_FOLDER}/data/{selected_season}/{selected_competition}/processed\"\n",
    "\n",
    "            # Find all frames parquet files\n",
    "            match_paths = glob.glob(os.path.join(DATA_FOLDER_PROCESSED, \"*.parquet\"))\n",
    "\n",
    "            # Extract IDs without the \".parquet\" extension\n",
    "            match_ids = [os.path.splitext(os.path.basename(path))[0] for path in match_paths]\n",
    "            # match_ids = ['49e6bfdf-abf3-499d-b60e-cf727c6523c1']\n",
    "\n",
    "            # For all matches\n",
    "            for match_id in match_ids:\n",
    "                # Convert parquet file to a DataFrame\n",
    "                file_path_match = f\"{DATA_FOLDER_PROCESSED}/{match_id}.parquet\"\n",
    "                frames_df = pd.read_parquet(file_path_match)\n",
    "                \n",
    "                # Append the DataFrame to frames_dfs\n",
    "                frames_dfs.append(frames_df)\n",
    "\n",
    "    return frames_dfs\n",
    "\n",
    "# Load every frames_df to a list\n",
    "frames_dfs = load_all_processed_frames()\n",
    "\n",
    "# Create an internal match_id for each game\n",
    "match_ids = range(len(frames_dfs))\n",
    "\n",
    "# Split match IDs into train, test, and validation sets\n",
    "train_ids, test_ids, val_ids = split_match_ids(match_ids=match_ids)\n",
    "\n",
    "# Select frames data for training, testing, and validation\n",
    "train_frames_dfs = [frames_dfs[i] for i in train_ids]\n",
    "test_frames_dfs = [frames_dfs[i] for i in test_ids]\n",
    "val_frames_dfs = [frames_dfs[i] for i in val_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196e924",
   "metadata": {},
   "source": [
    "### Load predictive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1746338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE: Always predict that all players will stand still\n",
    "# The calculations are based on x, y\n",
    "def predict_two_seconds_naive_static(frames_df):\n",
    "    frames_df['x_future_pred'] = frames_df['x']\n",
    "    frames_df['y_future_pred'] = frames_df['y']\n",
    "\n",
    "# NAIVE: Always predict that all players will continue with the same velocity\n",
    "# The calculations are based on x, y, v_x, and v_y\n",
    "def predict_two_seconds_naive_velocity(frames_df):\n",
    "    frames_df['x_future_pred'] = frames_df['x'] + frames_df['v_x'] * seconds_into_the_future\n",
    "    frames_df['y_future_pred'] = frames_df['y'] + frames_df['v_y'] * seconds_into_the_future\n",
    "\n",
    "# Make a prediction with a LSTM neural network model\n",
    "def predict_two_seconds_LSTM(frames_df):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "### Functions for calculating error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for distance wrongly predicted (in metres) for each object\n",
    "def add_pred_error(frames_df):\n",
    "    # Create a vector with the Eculidian distance between the true position and the predicted position\n",
    "    frames_df['pred_error'] = round(((frames_df['x_future_pred'] - frames_df['x_future'])**2 + (frames_df['y_future_pred'] - frames_df['y_future'])**2)**0.5, 2)\n",
    "    \n",
    "# Add a column for distance wrongly predicted (in metres) for each object. Also return average_pred_error\n",
    "def total_error_loss(frames_df, include_ball=False, ball_has_to_be_in_motion=True):\n",
    "    # Add 'pred_error' column if empty\n",
    "    if frames_df['pred_error'].empty:\n",
    "        add_pred_error(frames_df)\n",
    "    \n",
    "    # Create a new column to store modified pred_error values\n",
    "    frames_df['pred_error_tmp'] = frames_df['pred_error']\n",
    "    \n",
    "    # If specified, set pred_error to None for frames where the ball is not in motion\n",
    "    if ball_has_to_be_in_motion:\n",
    "        frames_df.loc[frames_df[\"ball_in_motion\"] != True, 'pred_error_tmp'] = None\n",
    "\n",
    "    # If specified, set pred_error to None for rows where 'team' is 'ball'\n",
    "    if not include_ball:\n",
    "        frames_df.loc[frames_df['team'] == 'ball', 'pred_error_tmp'] = None\n",
    "\n",
    "    # Calculate average pred_error_tmp, excluding rows where pred_error is None\n",
    "    average_pred_error = frames_df['pred_error_tmp'].mean()\n",
    "\n",
    "    # Drop the temporary column\n",
    "    frames_df.drop(columns=['pred_error_tmp'], inplace=True)\n",
    "\n",
    "    return round(average_pred_error, 2)\n",
    "\n",
    "# Calculate the average error for a list of games\n",
    "def calculate_average_error(frames_dfs, predict_function, include_ball, ball_has_to_be_in_motion):\n",
    "    # Predict the future positions\n",
    "    [predict_function(frames_df) for frames_df in frames_dfs]\n",
    "    [add_pred_error(frames_df) for frames_df in frames_dfs]\n",
    "\n",
    "    # Concatenate all frames dataframes into a single dataframe\n",
    "    concatted_frames_df = pd.concat(frames_dfs)    \n",
    "    \n",
    "    # Calculate the total error loss\n",
    "    error = total_error_loss(concatted_frames_df, include_ball, ball_has_to_be_in_motion)\n",
    "    \n",
    "    return error\n",
    "\n",
    "# Find a frame with approximatly the same error as the average_pred_error, with an interval\n",
    "def find_frame_with_average_error(frames_df, average_pred_error, error_margin):\n",
    "    # For all frames\n",
    "    frames = frames_df['frame'].unique()\n",
    "    for frame in frames:\n",
    "        current_error = frames_df[frames_df['frame'] == frame]['pred_error'].mean()\n",
    "        # If the current error is within the error_margin,\n",
    "        if (current_error >= average_pred_error - error_margin) and (current_error <= average_pred_error + error_margin):\n",
    "            # Return the result\n",
    "            return frame\n",
    "\n",
    "    # If no frame was found\n",
    "    print(f\"No frame found within the error margin of {error_margin}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863d61c",
   "metadata": {},
   "source": [
    "## Evaulate models\n",
    "### Evaluate the NAIVE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75155a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model 1\n",
    "error_naive_static = calculate_average_error(test_frames_dfs, predict_two_seconds_naive_static, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "print(f\"Model error naive static: {error_naive_static}\")\n",
    "\n",
    "# Test model 2\n",
    "error_naive_velocity = calculate_average_error(test_frames_dfs, predict_two_seconds_naive_velocity, include_ball=False, ball_has_to_be_in_motion=True)\n",
    "print(f\"Model error naive velocity: {error_naive_velocity}\")\n",
    "\n",
    "# Visualize one frame with average error for the first model, together with the corresponding frame for the second model\n",
    "frames_df = test_frames_dfs[0]\n",
    "frame_with_average_error = find_frame_with_average_error(frames_df, error_naive_static, error_margin=0.1)\n",
    "visualize_frame_prediction(frames_df, frame_with_average_error, \"naive_static\")\n",
    "visualize_frame_prediction(frames_df, frame_with_average_error, \"naive_velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e72fe",
   "metadata": {},
   "source": [
    "### Evaulate the NAIVE models with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction functions (models) you want to test\n",
    "prediction_functions = {\n",
    "    \"Naive Static\": predict_two_seconds_naive_static,\n",
    "    \"Naive Velocity\": predict_two_seconds_naive_velocity\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Define the combinations of include_ball and ball_has_to_be_in_motion\n",
    "combinations = [(True, True), (True, False), (False, True), (False, False)]\n",
    "\n",
    "# Load all processed frames\n",
    "frames_dfs = load_all_processed_frames()\n",
    "\n",
    "# Loop through each combination\n",
    "for include_ball, ball_has_to_be_in_motion in combinations:\n",
    "    # Add the combination of parameters\n",
    "    result = {\"Include Ball\": include_ball, \"Ball in Motion\": ball_has_to_be_in_motion}\n",
    "    # Loop through each prediction function (model)\n",
    "    for model_name, predict_function in prediction_functions.items():\n",
    "        # Calculate average error for the current prediction function (model)\n",
    "        avg_error = calculate_average_error(frames_dfs, predict_function, include_ball, ball_has_to_be_in_motion)\n",
    "        result[model_name] = avg_error\n",
    "    \n",
    "    # Append the results to the list\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
