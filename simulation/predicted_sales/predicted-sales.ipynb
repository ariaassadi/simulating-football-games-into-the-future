{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c078666b",
   "metadata": {},
   "source": [
    "# Notebook for training predictive models\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43b454-7b17-42fc-be8a-de1770ba3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 19:53:53.105930: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, BatchNormalization, Dropout, Reshape, LSTM\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import date\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from predicted_sales_utils import split_match_ids, get_next_model_filename, euclidean_distance_loss, total_error_loss, define_regularizers, embedding_config, prepare_EL_input_data, prepare_LSTM_input_data, create_embeddings, smooth_predictions_xy, run_model, evaluate_model, print_column_variance\n",
    "from predicted_sales_utils import prepare_df, add_can_be_sequentialized, extract_variables, load_tf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf81b0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfca7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical, categorical, and y columns\n",
    "numerical_cols = []\n",
    "categorical_cols = []\n",
    "\n",
    "# Define parameters for model training\n",
    "n_epochs = 1\n",
    "batch_size = 32\n",
    "sequence_length = 3    # Sequence length for LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef43b7",
   "metadata": {},
   "source": [
    "### Load frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98345c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from parquet and perform basic filtering\n",
    "def load_data():\n",
    "  # Load sales_per_month_df from parquet\n",
    "  sales_per_month_df = pd.read_parquet('Predicted-sales.parquet')\n",
    "\n",
    "  # Remove 'Övrigt' entries\n",
    "  sales_per_month_df = sales_per_month_df[sales_per_month_df['Product'] != 'Övrigt']\n",
    "  sales_per_month_df = sales_per_month_df[sales_per_month_df['Machine Sub Group'] != 'Övrigt']\n",
    "\n",
    "  return sales_per_month_df\n",
    "\n",
    "# Create a train, val, and test split based on all 'Machine ID's\n",
    "def train_test_val_split_by_machine_id(df, test_size=0.2, val_size=0.2):\n",
    "    # Create unique list of Machine IDs\n",
    "    machine_ids = df['Machine ID'].unique()\n",
    "\n",
    "    # Split Machine IDs into train and test+val sets\n",
    "    train_ids, test_val_ids = train_test_split(machine_ids, test_size=test_size+val_size, random_state=42)\n",
    "\n",
    "    # Split test+val into test and val sets\n",
    "    test_ids, val_ids = train_test_split(test_val_ids, test_size=val_size/(test_size+val_size), random_state=42)\n",
    "\n",
    "    # Assign records to train, test, or val sets based on Machine ID\n",
    "    train_df = df[df['Machine ID'].isin(train_ids)]\n",
    "    test_df = df[df['Machine ID'].isin(test_ids)]\n",
    "    val_df = df[df['Machine ID'].isin(val_ids)]\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Load the data and create three DataFrames\n",
    "sales_per_month_df = load_data()\n",
    "\n",
    "# Remove 'Övrigt' products\n",
    "sales_per_month_df = sales_per_month_df[sales_per_month_df['Product'] != 'Övrigt']\n",
    "sales_per_month_df = sales_per_month_df[sales_per_month_df['Machine Group'] != 'Övrigt']\n",
    "sales_per_month_df['Product Name'] = sales_per_month_df['Product']\n",
    "sales_per_month_df = sales_per_month_df[sales_per_month_df['Product'] == 'Cloetta Kexchoklad']\n",
    "\n",
    "train_df, val_df, test_df = train_test_val_split_by_machine_id(sales_per_month_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8128c89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Product', 'Machine ID', 'Timestep', 'Sales (kr)', 'Price',\n",
       "       'Number of Sales', 'Category', 'Year', 'Month', 'Sales Next Month (kr)',\n",
       "       'New Product', 'Total Sales (kr)', 'Dryck Sales (kr)', 'Mat Sales (kr)',\n",
       "       'Snacks Sales (kr)', 'Sport Sales (kr)', 'Bil Sales (kr)',\n",
       "       'Naive Static (kr)', 'Error Naive Static (%)', 'Naive Static Plus (kr)',\n",
       "       'Error Naive Static Plus (%)', 'Naive Sequential (kr)',\n",
       "       'Error Naive Sequential (%)', 'Naive Sequential Plus (kr)',\n",
       "       'Error Naive Sequential Plus (%)', 'Machine Group', 'Machine Sub Group',\n",
       "       'Machine Model', 'Average Increase', 'Product Name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24a32",
   "metadata": {},
   "source": [
    "## Predictive model 1\n",
    "### NN with Embedding layers\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005fb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the neural network model with embeddings layers\n",
    "def define_NN_model(numerical_input_shape, categorical_cols, l1=0, l2=0):\n",
    "    # Inputs for each categorical feature\n",
    "    categorical_inputs = []\n",
    "    categorical_flats = []\n",
    "    for col in categorical_cols:\n",
    "        # Replace spaces with underscores in the input name\n",
    "        input_name = f'input_{col.replace(\" \", \"_\")}'\n",
    "        embedding_name = f'embedding_{col.replace(\" \", \"_\")}'\n",
    "\n",
    "        cat_input = Input(shape=(1,), name=input_name)  # Input for each categorical feature\n",
    "        emb_layer = Embedding(\n",
    "            input_dim=embedding_config[col]['n_categories'],\n",
    "            output_dim=embedding_config[col]['output_dim'],\n",
    "            input_length=1,\n",
    "            name=embedding_name\n",
    "        )(cat_input)\n",
    "        flat_layer = Flatten()(emb_layer)\n",
    "        categorical_inputs.append(cat_input)\n",
    "        categorical_flats.append(flat_layer)\n",
    "\n",
    "    # Prepare input layer for numerical data\n",
    "    numerical_input = Input(shape=(numerical_input_shape,), name='numerical_input')\n",
    "\n",
    "    # Concatenate all flattened embeddings with the numerical input\n",
    "    concatenated_features = Concatenate()([*categorical_flats, numerical_input]) if categorical_flats else numerical_input\n",
    "\n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "    output_layer = Dense(1, name='output_layer')(dense_layer_2)  # Output layer 'Sales Next Month (kr)'\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[*categorical_inputs, numerical_input], outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train NN model\n",
    "def train_NN_model(train_df, val_df, numerical_cols, categorical_cols, l1=0, l2=0, special_text=None):\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_EL_input_data(train_df, numerical_cols, categorical_cols)\n",
    "    X_val_input, y_val = prepare_EL_input_data(val_df, numerical_cols, categorical_cols)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_NN_model(len(numerical_cols), categorical_cols, l1, l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"pSales_NN\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078b2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NN model with embedding layers\n",
    "# numerical_cols = ['Price', 'Number of Sales', 'Category', 'New Product', 'Total Sales (kr)', 'Dryck Sales (kr)', 'Mat Sales (kr)', 'Snacks Sales (kr)', 'Sport Sales (kr)', 'Bil Sales (kr)']\n",
    "n_epochs = 3\n",
    "categorical_cols = ['Category', 'Machine Group', 'Month']\n",
    "numerical_cols = ['Price', 'Number of Sales', 'Total Sales (kr)', 'Dryck Sales (kr)', 'Mat Sales (kr)', 'Snacks Sales (kr)', 'Sport Sales (kr)', 'Bil Sales (kr)', 'Average Increase']\n",
    "\n",
    "# train_NN_model(train_df, val_df, numerical_cols, categorical_cols, l2=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58786e12",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "696c04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print test results\n",
    "# model_names = [f'pSales_NN_v{i}' for i in range(5, 6)]\n",
    "# for model_name in model_names:\n",
    "#     error = evaluate_model(test_df, model_name)\n",
    "#     print(f\"{model_name}: {error}\")\n",
    "\n",
    "# Test different alpha values\n",
    "# model_name = 'pSales_NN_v3'\n",
    "# test_df = run_model(test_df, model_name)\n",
    "# for alpha in [0.9, 0.93, 0.96, 0.99, 1]:\n",
    "#     test_df = test_df.copy()\n",
    "#     test_df = smooth_predictions_xy(test_df, model_name, alpha=alpha)\n",
    "#     error = total_error_loss(test_df, model_name)\n",
    "#     print(f\"{round(alpha, 2)}: {error}\")\n",
    "\n",
    "# # Print column variance\n",
    "# model_name = 'pSales_NN_v3'\n",
    "# print_column_variance(test_df, model_name, 'Machine Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ccb12",
   "metadata": {},
   "source": [
    "## Predictive model 2\n",
    "### LSTM model\n",
    "Player-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46e9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the LSTM model with embeddings layers\n",
    "def define_LSTM_model(numerical_input_shape, categorical_cols, sequence_length, l1=0, l2=0):\n",
    "    categorical_inputs = []\n",
    "    categorical_flats = []\n",
    "    \n",
    "    # Create inputs for each categorical feature\n",
    "    for col in categorical_cols:\n",
    "        input_name = f'input_{col.replace(\" \", \"_\")}'\n",
    "        embedding_name = f'embedding_{col.replace(\" \", \"_\")}'\n",
    "\n",
    "        cat_input = Input(shape=(1,), name=input_name)\n",
    "        emb_layer = Embedding(\n",
    "            input_dim=embedding_config[col]['n_categories'],\n",
    "            output_dim=embedding_config[col]['output_dim'],\n",
    "            input_length=1,\n",
    "            name=embedding_name\n",
    "        )(cat_input)\n",
    "        flat_layer = Flatten()(emb_layer)\n",
    "        categorical_inputs.append(cat_input)\n",
    "        categorical_flats.append(flat_layer)\n",
    "\n",
    "    # Prepare input layer for sequential numerical data\n",
    "    numerical_input = Input(shape=(sequence_length, numerical_input_shape), name='numerical_input')\n",
    "    lstm_layer = LSTM(64, return_sequences=False, name='lstm_numerical')(numerical_input)\n",
    "\n",
    "    # Concatenate embeddings with numerical input\n",
    "    if categorical_flats:\n",
    "        concatenated_features = Concatenate()([*categorical_flats, lstm_layer])\n",
    "    else:\n",
    "        concatenated_features = lstm_layer  # Only use LSTM output if no categorical data\n",
    "\n",
    "    # Dense layers\n",
    "    regularizer = define_regularizers(l1, l2)  # Set regularizer\n",
    "    dense_layer_1 = Dense(64, activation='relu', kernel_regularizer=regularizer)(concatenated_features)\n",
    "    dense_layer_2 = Dense(32, activation='relu', kernel_regularizer=regularizer)(dense_layer_1)\n",
    "    output_layer = Dense(1, name='output_layer')(dense_layer_2)  # Output layer 'Sales Next Month (kr)'\n",
    "\n",
    "    # Building the model\n",
    "    model = Model(inputs=[*categorical_inputs, numerical_input], outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_LSTM_model(train_df, val_df, numerical_cols, categorical_cols, sequence_length, l1=0, l2=0, special_text=None):\n",
    "    # Prepare inputs\n",
    "    X_train_input, y_train = prepare_LSTM_input_data(train_df, numerical_cols, categorical_cols, sequence_length)\n",
    "    X_val_input, y_val = prepare_LSTM_input_data(val_df, numerical_cols, categorical_cols, sequence_length)\n",
    "\n",
    "    # Define the model\n",
    "    model = define_LSTM_model(len(numerical_cols), categorical_cols, sequence_length, l1, l2)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "    # Train the model with the corrected input format\n",
    "    history = model.fit(X_train_input, y_train, validation_data=(X_val_input, y_val), epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Save the trained model to disk\n",
    "    model_filename = get_next_model_filename(\"pSales_LSTM\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Generate the corresponding txt filename\n",
    "    output_txt_filename = os.path.splitext(model_filename)[0] + \".txt\"\n",
    "\n",
    "    # Write the output directly to the txt file\n",
    "    with open(output_txt_filename, 'w') as f:\n",
    "        # Write the some general info at the begging of the file\n",
    "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "        f.write(f\"{today_date}\\n\")\n",
    "        f.write(f\"epochs={n_epochs}\\n\")\n",
    "        f.write(f\"sequence_length={sequence_length}\\n\")\n",
    "        f.write(f\"numerical_cols={numerical_cols}\\n\")\n",
    "        f.write(f\"categorical_cols={categorical_cols}\\n\")\n",
    "        if l1 != 0: f.write(f\"l1={l1}\\n\")\n",
    "        if l2 != 0: f.write(f\"l2={l2}\\n\")\n",
    "        if special_text: f.write(f\"{special_text}\\n\")\n",
    "\n",
    "        # Write the training results\n",
    "        f.write(\"\\nTraining results:\\n\")\n",
    "        for key, value in history.history.items():\n",
    "            rounded_values = [round(v, 2) for v in value]\n",
    "            f.write(f\"{key}: {rounded_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc576cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_df = test_df[(test_df['Machine ID'] == 997510210) & (test_df['Product'] == 'Vitamin Well Refresh') & (test_df['Month'] < 7)]\n",
    "\n",
    "# sequence_length = 3\n",
    "# categorical_cols = ['Category', 'Month']\n",
    "# numerical_cols = ['Price', 'Number of Sales']\n",
    "\n",
    "# X_small_input, y_small = prepare_LSTM_input_data(small_df, numerical_cols, categorical_cols, sequence_length)\n",
    "\n",
    "# X_small_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5140068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 19:54:00.143460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38551 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:e3:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 19:54:03.485975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2024-04-23 19:54:03.590518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-04-23 19:54:03.592062: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x14dd440c6dc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-23 19:54:03.592079: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2024-04-23 19:54:03.596563: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-23 19:54:03.703868: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 - 4s - loss: 230.6442 - val_loss: 265.3925 - 4s/epoch - 60ms/step\n",
      "Epoch 2/3\n",
      "67/67 - 0s - loss: 171.4241 - val_loss: 193.8450 - 168ms/epoch - 3ms/step\n",
      "Epoch 3/3\n",
      "67/67 - 0s - loss: 133.6713 - val_loss: 154.5726 - 166ms/epoch - 2ms/step\n",
      "Epoch 1/3\n",
      "59/59 - 3s - loss: 234.7517 - val_loss: 276.1885 - 3s/epoch - 44ms/step\n",
      "Epoch 2/3\n",
      "59/59 - 0s - loss: 180.0569 - val_loss: 207.7946 - 153ms/epoch - 3ms/step\n",
      "Epoch 3/3\n",
      "59/59 - 0s - loss: 137.7166 - val_loss: 160.5589 - 154ms/epoch - 3ms/step\n",
      "Epoch 1/3\n",
      "46/46 - 3s - loss: 247.3797 - val_loss: 307.7054 - 3s/epoch - 55ms/step\n",
      "Epoch 2/3\n",
      "46/46 - 0s - loss: 228.2562 - val_loss: 265.6542 - 124ms/epoch - 3ms/step\n",
      "Epoch 3/3\n",
      "46/46 - 0s - loss: 177.9030 - val_loss: 224.7046 - 123ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "n_epochs = 3\n",
    "sequence_length = 3\n",
    "categorical_cols = []\n",
    "numerical_cols = ['Price', 'Number of Sales', 'Total Sales (kr)']\n",
    "\n",
    "train_LSTM_model(train_df, val_df, numerical_cols, categorical_cols, sequence_length, l2=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec88142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 917us/step\n",
      "pSales_LSTM_v4: 37.26\n",
      "21/21 [==============================] - 0s 946us/step\n",
      "pSales_LSTM_v5: 36.48\n",
      "17/17 [==============================] - 0s 950us/step\n",
      "pSales_LSTM_v6: 45.71\n"
     ]
    }
   ],
   "source": [
    "# Print test results\n",
    "model_names = [f'pSales_LSTM_v{i}' for i in range(4, 7)]\n",
    "for model_name in model_names:\n",
    "    error = evaluate_model(test_df, model_name)\n",
    "    print(f\"{model_name}: {error}\")\n",
    "\n",
    "# Test different alpha values\n",
    "# model_name = 'pSales_NN_v3'\n",
    "# test_df = run_model(test_df, model_name)\n",
    "# for alpha in [0.9, 0.93, 0.96, 0.99, 1]:\n",
    "#     test_df = test_df.copy()\n",
    "#     test_df = smooth_predictions_xy(test_df, model_name, alpha=alpha)\n",
    "#     error = total_error_loss(test_df, model_name)\n",
    "#     print(f\"{round(alpha, 2)}: {error}\")\n",
    "\n",
    "# # Print column variance\n",
    "# model_name = 'pSales_NN_v3'\n",
    "# print_column_variance(test_df, model_name, 'Machine Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d70d0",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize training results\n",
    "# model_name = 'NN_embedding_model_3'\n",
    "# training_results = {\n",
    "#     'loss': [2.0478146076202393, 2.0088889598846436, 2.0007753372192383, 1.9968146085739136, 1.9937269687652588, 1.9921172857284546, 1.990675687789917, 1.9893001317977905, 1.9881930351257324, 1.9875684976577759, 1.9872304201126099, 1.9865171909332275, 1.9859004020690918, 1.985435128211975, 1.9848004579544067, 1.983401894569397, 1.9824390411376953, 1.9820188283920288, 1.981824517250061, 1.9817743301391602],\n",
    "#     'val_loss': [4.535243034362793, 4.51762580871582, 4.469428539276123, 4.436275482177734, 4.456634521484375, 4.815524578094482, 4.3103556632995605, 4.498797416687012, 4.790141582489014, 4.464589595794678, 4.674554347991943, 4.561259746551514, 4.533383369445801, 4.472135066986084, 4.466953754425049, 4.478504180908203, 4.723540782928467, 4.859069347381592, 4.496937274932861, 4.377903461456299]\n",
    "# }\n",
    "\n",
    "# visualize_training_results(training_results, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Feb  1 2024, 03:10:29) [GCC 11.3.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
